---
title: "tech_analysis_p131"
format: html
editor: visual
---

## Technical Analysis of Product and Customer News Sources on the Web


Most text mining and natural language processing (NLP) modeling uses 'bag of words' or 'bag of n-grams' methods. Despite their simplicity, these models usually demonstrate good performance on text categorization and classification tasks. But in contrast to their theoretical simplicity and practical efficiency, building bag-of-words models involves technical challenges. This is especially the case in R because of its copy-on-modify semantics.

Let’s briefly review some of the steps in a typical text analysis pipeline:

1. The auditor usually begins by constructing a document-term matrix (DTM) or term-co-occurrence matrix (TCM) from input documents. In other words, the first step is to vectorize text by creating a map from words or n-grams to a vector space.

1. The auditor fits a model to that DTM. These models might include text classification, topic modeling, similarity search, etc. Fitting the model will include tuning and validating the model.

1. Finally the auditor applies the model to new data.

Texts themselves can take up a lot of memory, but vectorized texts usually do not, because they are stored as sparse matrices. Because of R’s copy-on-modify semantics, it is not easy to iteratively grow a DTM; constructing a DTM, even for a small collections of documents, can be a serious bottleneck. It involves reading the whole collection of text documents into memory and processing it as single vector, which can easily increase memory use by a factor of two to four. The `text2vec` package solves this problem by providing a better way of constructing a document-term matrix.

As an example of NLP using the `text2vec` package, I will parse a dataset that comes with the package -- the `movie_review` dataset. It consists of 5000 movie reviews, each of which is marked as positive or negative.  First, split the dataset into two parts - train and test. 


```{r eval=T,message=F,error=F, warning=F}

library(text2vec)
library(data.table)
library(magrittr)
data("movie_review")
setDT(movie_review)
setkey(movie_review, id)
set.seed(2017L)
all_ids = movie_review$id
train_ids = sample(all_ids, 4000)
test_ids = setdiff(all_ids, train_ids)
train = movie_review[J(train_ids)]
test = movie_review[J(test_ids)]

head(movie_review)
class(movie_review)
```



### Vocabulary-based vectorization

To represent documents in vector space, we first have to create 'term' mappings. We call them terms instead of words because they can be, not just single words, but arbitrary n-grams -- contiguous sequence of n items, where items can be phonemes, syllables, letters, words or base pairs.  We represent a set of documents as a sparse matrix, where each row corresponds to a document and each column corresponds to a term. Create a vocabulary-based DTM by collecting unique terms from all documents and mark each of them with a unique ID using the `create_vocabulary()` function, using an iterator to create the vocabulary.

The following code chunk:

1.  creates an iterator over tokens with the `itoken()` function. All functions prefixed with `create_ work` with these iterators. R users might find this idiom unusual, but the iterator abstraction allows us to hide most of details about input and to process data in memory-friendly chunks.

1.  builds the vocabulary with the `create_vocabulary()` function.


```{r eval=T,message=F,error=F, warning=F}


# define preprocessing function and tokenization function
prep_fun = tolower
tok_fun = word_tokenizer

it_train = itoken(train$review, 
             preprocessor = prep_fun, 
             tokenizer = tok_fun, 
             ids = train$id, 
             progressbar = FALSE)
vocab = create_vocabulary(it_train)

```



Alternatively, we could create list of tokens and reuse it in further steps. Each element of the list should represent a document, and each element should be a character vector of tokens.


```{r eval=T,message=F,error=F, warning=F}


library(magrittr)
library(tidyverse)
library(text2vec)

train_tokens = train$review %>% 
  prep_fun %>% 
  tok_fun
it_train = itoken(train_tokens, 
                  ids = train$id,
                  # turn off progressbar because it won't look nice in rmd
                  progressbar = FALSE)

vocab = create_vocabulary(it_train)
vocab

vectorizer = vocab_vectorizer(vocab)
t1 = Sys.time()
dtm_train = create_dtm(it_train, vectorizer)
print(difftime(Sys.time(), t1, units = 'sec'))

dim(dtm_train)

identical(rownames(dtm_train), train$id)

``` 


Once we have a vocabulary, we can construct a document-term matrix.

```{r eval=T,message=F,error=F, warning=F}

vectorizer = vocab_vectorizer(vocab)
t1 = Sys.time()
dtm_train = create_dtm(it_train, vectorizer)
print(difftime(Sys.time(), t1, units = 'sec'))
```


At this point, we are ready to fit our model. Here we will use the `glmnet` (Lasso and Elastic-Net Regularized Generalized Linear Models) package to fit a logistic regression model with an L1 penalty (LASSO = least absolute shrinkage and selection operator) and 4-fold cross-validation.

```{r eval=T,message=F,error=F, warning=F}


library(glmnet)
NFOLDS = 4
t1 = Sys.time()
glmnet_classifier = 
  cv.glmnet(x = dtm_train, 
            y = train[['sentiment']], 
                              family = 'binomial',  
                              
##  for the movie_reviews this is "binomial", 
## for the food reviews it is 1-5, so "multinomial" 

                             # L1 penalty
                              alpha = 1,
                              # interested in the area under ROC curve
                              type.measure = "auc",
                              # 5-fold cross-validation
                              nfolds = NFOLDS,
                              # high value is less accurate, but has faster training
                              thresh = 1e-3,
                              # again lower number of iterations for faster training
                              maxit = 1e3)
print(difftime(Sys.time(), t1, units = 'sec'))

plot(glmnet_classifier)



print(paste("max AUC =", round(max(glmnet_classifier$cvm), 4)))


```


We have successfully fit a model to our DTM. Now we can check the model’s performance on test data. Note that we use exactly the same functions from prepossessing and tokenization. Also we reuse/use the same vectorizer - function which maps terms to indices.

```{r eval=T,message=F,error=F, warning=F}

# Note that most text2vec functions are pipe friendly!
it_test = test$review %>% 
  prep_fun %>% tok_fun %>% 
  # turn off progressbar because it won't look nice in rmd
  itoken(ids = test$id, progressbar = FALSE)
         

dtm_test = create_dtm(it_test, vectorizer)

preds = predict(glmnet_classifier, dtm_test, type = 'response')[,1]
glmnet:::auc(test$sentiment, preds)
```


The result shows that performance on the test data is roughly the same as we expected from cross-validation.  Note though that the training time for the model was high. We can reduce it and also significantly improve accuracy by 'pruning' the vocabulary.  For example, we can find words “a”, “the”, “in”, “I”, “you”, “on”, etc in almost all documents, but they do not provide much useful information. Usually such words are called 'stop words'. On the other hand, the corpus also contains very uncommon terms, which are contained in only a few documents. These terms are also useless, because we don’t have sufficient statistics for them. Here we will remove pre-defined stopwords, very common and very unusual terms.

```{r eval=T,message=F,error=F, warning=F}


stop_words = c("i", 
               "me", 
               "my", 
               "myself", 
               "we", 
               "our", 
               "ours", 
               "ourselves", 
               "you", 
               "your", 
               "yours")

t1 = Sys.time()
vocab = create_vocabulary(it_train, stopwords = stop_words)
print(difftime(Sys.time(), t1, units = 'sec'))

pruned_vocab = prune_vocabulary(vocab, 
                                 term_count_min = 10, 
                                 doc_proportion_max = 0.5,
                                 doc_proportion_min = 0.001)
vectorizer = vocab_vectorizer(pruned_vocab)


# create dtm_train with new pruned vocabulary vectorizer

t1 = Sys.time()
dtm_train  = create_dtm(it_train, vectorizer)
print(difftime(Sys.time(), t1, units = 'sec')) 


dim(dtm_train)

# create DTM for test data with the same vectorizer:

dtm_test = create_dtm(it_test, vectorizer)
dim(dtm_test)

```


This model can be improved by using n-grams instead of words -- in the following code-chunk, I use up to 2-grams:

```{r eval=T,message=F,error=F, warning=F}

t1 = Sys.time()
vocab = create_vocabulary(it_train, ngram = c(1L, 2L))
print(difftime(Sys.time(), t1, units = 'sec'))


vocab = prune_vocabulary(vocab, term_count_min = 10, 
                         doc_proportion_max = 0.5)

bigram_vectorizer = vocab_vectorizer(vocab)

dtm_train = create_dtm(it_train, bigram_vectorizer)

t1 = Sys.time()
glmnet_classifier = cv.glmnet(x = dtm_train, y = train[['sentiment']], 
                 family = 'binomial', 
                 alpha = 1,
                 type.measure = "auc",
                 nfolds = NFOLDS,
                 thresh = 1e-3,
                 maxit = 1e3)
print(difftime(Sys.time(), t1, units = 'sec'))


plot(glmnet_classifier)


print(paste("max AUC =", round(max(glmnet_classifier$cvm), 4)))


# apply vectorizer
dtm_test = create_dtm(it_test, bigram_vectorizer)

preds = 
  predict(glmnet_classifier, 
          dtm_test,
          type = 'response')[,1]

glmnet:::auc(test$sentiment, preds)


```


To further improve performance, we can use 'feature hashing' which achieves greater speed by avoiding a lookup over an associative array. Another benefit is that it leads to a very low memory footprint, since we can map an arbitrary number of features into much more compact space, using `text2vec`.  The method often makes AUC slightly worse in exchange for improved execution times, which on large collections of documents can provide a significant advantage.


```{r eval=T,message=F,error=F, warning=F}


h_vectorizer = hash_vectorizer(hash_size = 2 ^ 14, ngram = c(1L, 2L))

t1 = Sys.time()
dtm_train = create_dtm(it_train, h_vectorizer)
print(difftime(Sys.time(), t1, units = 'sec'))


t1 = Sys.time()
glmnet_classifier = cv.glmnet(x = dtm_train, y = train[['sentiment']], 
                             family = 'binomial', 
                             alpha = 1,
                             type.measure = "auc",
                             nfolds = 5,
                             thresh = 1e-3,
                             maxit = 1e3)
print(difftime(Sys.time(), t1, units = 'sec'))


plot(glmnet_classifier)


print(paste("max AUC =", round(max(glmnet_classifier$cvm), 4)))


dtm_test = create_dtm(it_test, h_vectorizer)

preds = 
  predict(glmnet_classifier, 
          dtm_test , 
          type = 'response')[, 1]

glmnet:::auc(test$sentiment, preds)


```


Before analysis it can be useful to transform DTM, using the `normalize` function, since lengths of the documents in collection can significantly vary. Normalization transforms the rows of DTM so we adjust values measured on different scales to a not to transform rows so that the sum of the row values equals 1.  There is also a `TfIdf` function which not only normalizes DTM, but also increase the weight of terms which are specific to a single document or handful of documents and decrease the weight for terms used in most documents.  Both can further improve execution time and accuracy, which may be of value in very large datasets.