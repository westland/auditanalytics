---
title: "uncollectables_p287"
format: html
editor: visual
---


### Estimation and Accrual of the Allowance for Doubtful Accounts

The Allowance for Doubtful (uncollectable) Accounts is a contra-account, i.e., it offsets the accounts receivable account balance to calculate the net revenues that have been earned.  Some of the accounts receivable at year-end will not be paid, but we don't know which of these will not be paid (otherwise, the client probably would not have sold to this customer).  These are expenses which must be reported as a contra-account (Cr balance) to the accounts receivable account (Dr balance).  The estimation policy that the client applies will be based on historical payments and unpaid debt write-offs. 

The following code chunks present examples of predictive algorithms to estimate the future collection of accounts receivable.  Though there are various approaches to estimating when customers will pay their accounts, if at all.  I adopt a 'time to payment' estimate, where uncollectable accounts will theoretically have an infinite time to payment.  At year end, the longer an outstanding receivable has remained unpaid, the more likely we are to decide that the time to payment will be infinite.  Just as with traditional methods for estimating the Allowance for Uncollectable A/R, I apply a particular decision model for estimating uncollectables based on the predicted aging of A/R


```{r eval=T, error=F, message=F, warning=F, fig.cap="Linear Model of Time to Payment, Residuals QQ Plot"}

library(tidyverse)
library(ggplot2)
library(lubridate)

sales_journal[is.na(sales_journal)] <- 0

credit_sales_journal <- 
  sales_journal[,1:15] %>% 
  filter(cash_not_ar == 0) %>%    ## credit only
  mutate(collection_delay =
           .01 + time_length(        ## add .01 to get rid of 0's so logs can be computed 
             ymd(collection_date) -
             ymd(invoice_date),
             unit = "day"), 
         customer_no =
           as.character(customer_no)
         ) %>% 
  as.data.frame()


## Simple OLS regression

l_mod <- 
  lm(collection_delay ~
        customer_no +
        unit_cost +
        sales_unit_price +
        sales_count +
        sales_return,
        data=credit_sales_journal)

summary(l_mod)


sq <- seq(1,length(l_mod$residuals))
res <- data.frame(sq,l_mod$residuals)

ggplot(res, aes(sample=l_mod$residuals)) +
  stat_qq() +
  stat_qq_line()+
  labs(title = "Linear Model of Time to Payment, Residuals QQ Plot")

```


### GLM-Exponential Regression

Time to payment can be better modeled using an exponential distribution, which is commonly used to model waiting times for Poisson processes.  The exponential distribution is a specific Gamma distribution where the dispersion parameter (which is what distinguishes an exponential distribution from the more general Gamma distribution) does not affect the parameter estimates in a generalized linear model, only the standard errors of the parameters/confidence intervals/p-values etc. The Gamma family is parametrized in glm() by two parameters: mean and dispersion; the "dispersion" regulates the shape.  Therefore fit a GLM with the Gamma family, and then produce a "summary" with dispersion parameter set equal to 1, since this value corresponds to the exponential distribution in the Gamma family.

```{r eval=T, error=F, message=F, warning=F, fig.cap="GLM Model of Time to Payment, Residuals QQ Plot"}



exp_mod <- 
  glm(collection_delay ~
        customer_no +
        unit_cost +
        sales_unit_price +
        sales_count +
        sales_return,
        data=credit_sales_journal, 
      family = Gamma(link = "inverse")
  )

sq <- seq(1,length(exp_mod$residuals))
res <- data.frame(sq,exp_mod$residuals)

ggplot(res, aes(sample=exp_mod$residuals)) +
  stat_qq() +
  stat_qq_line() +
  labs(title = "GLM Model of Time to Payment, Residuals QQ Plot")
```

Reviewing the Q-Q plot of residuals for the GLM model with Gaussian fit and an inverse link shows that it tends to overcorrect for the misfit in the linear model.  Nonetheless, fit improves on a simple linear model.  How well does the GLM model fit the collection timeseries?  The following code chunk compares our predicted collections to actual collections.  There is substantially more variance in the actual transaction flows than the predicted flows (which is to be expected), but the predictions are centered at the same place as the actual collections, and over time, this should be sufficient to accurately estimate the allowance for doubtful accounts. 

```{r  fig.cap="GLM Model Predictions vs. Original Timeseries"}

## how well does the glm model predict?


predictors <- 
  credit_sales_journal %>% 
  select(customer_no,
        unit_cost,
        sales_unit_price,
        sales_count,
        sales_return
         )

pred_collection_delay <- 
  1 / predict(exp_mod, dispersion=1, predictors)  

plt <- 
  credit_sales_journal %>% 
  select(invoice_date, collection_delay) %>% 
  cbind(pred_collection_delay)

ggplot() + 
  geom_line(data = plt, aes(x = invoice_date, y = collection_delay), color = "blue") +
  geom_line(data = plt, aes(x = invoice_date, y = pred_collection_delay), color = "red") +
  xlab('invoice_date') +
  ylab('delay (blue) vs. predicted (red)')


```


### Time series forecasting

There are many methods for forecasting time series which can be broadly categorized as either (1) econometric methods, such as ARIMA; (2) signal processing methods, such as Fourier Analysis; or (3) machine learning methods which involve recurrent neural networks (such as LSTM networks).  In the current section, I demonstrate some standard ARIMA models which improve on existing methods for accruing the Allowance for Uncollectable A/R. Machine learning models show promise in more completely extracting information from client records.  Forecasts from machine learning, or signal processing models can be used in the same fashion that is demonstrated here in the application of econometric models.  

If we assume that there are time dependent effects that influence the rate of payment, we could actually model payments history as an autoregressive integrated moving average (ARIMA) using R's time series tools, as shown in the code chunk below.   

```{r eval=T, error=F, message=F, warning=F, fig.cap="Customer Payment Performance"}

library(ggplot2)
library(lubridate)
library(forecast)
library(tidyquant)
library(timetk)
library(sweep)
library(tidyverse)

sales_journal[is.na(sales_journal)] <- 0

## create a time-series object

credit_sales <- 
  sales_journal[,1:15] %>% 
  filter(cash_not_ar == 0) %>%    ## credit only
  mutate(collection_delay =
             time_length(        
             ymd(collection_date) -
             ymd(invoice_date),
             unit = "day"), 
         customer_no =
           as.character(customer_no)
         ) %>% 
    as.data.frame() %>%
    group_by(collection_delay) %>% 
    summarise_if(is.numeric, sum, na.rm = TRUE) %>% 
    select(collection_amount) %>% 
    ts(frequency=365, start = c(2019,1))


moving_sales <- ma(credit_sales , order=30) ## 1 month = 30 day moving average
#plot(moving_sales)

plot(moving_sales,
  xlab="Time to Pay", 
  ylab="Amount Collected")

## Fit and forecast with auto.arima()
## The forecast package offers auto.arima() function to fit ARIMA models. It can also be manually fit using Arima(). A caveat with ARIMA models in R is that it does not have the functionality to fit long seasonality of more than 350 periods eg: 365 days for daily data or 24 hours for 15 sec data.

autoArimaFit <- auto.arima(credit_sales)
#plot(forecast(autoArimaFit, h=20))

plot(forecast(autoArimaFit, h=20),
  xlab="Time to Pay", 
  ylab="Amount Collected")


sales_journal[is.na(sales_journal)] <- 0

## create a time-series object

credit_sales <- 
  sales_journal[,1:15] %>% 
  filter(cash_not_ar == 0) %>%    ## credit only
  mutate(collection_delay =
             time_length(        
             ymd(collection_date) -
             ymd(invoice_date),
             unit = "day"), 
         customer_no =
           as.character(customer_no)
         ) %>% 
    as.data.frame() %>%
    group_by(customer_no, collection_delay) %>%
    summarise(collections = sum(collection_amount)) %>% 
  as.tibble()

credit_sales %>%
    ggplot(aes(x = collection_delay, y = collections, group = customer_no)) +
    geom_area(aes(fill = customer_no), position = "stack") +
    labs(title = "Customer Payment Performance", x = "Days to Pay", y = "Sales") +
    scale_y_continuous() +
    theme_tq()


```



### Forecasting Accounts Receivable Collections

The forecasting workflow involves a few basic steps:

- Step 1: Coerce to a ts object class.
- Step 2: Apply a model (or set of models)
- Step 3: Forecast the models (similar to predict)
- Step 4: Tidy the forecast

First, I need to get the data organized into groups by month of the year. 

```{r eval=F, error=F, message=F, warning=F, fig.cap="A/R Collection Delay by Customer"}


daily_collections_by_customer <-
    sales_journal[,1:15] %>% 
    filter(cash_not_ar == 0) %>%    ## credit only
    mutate(invoice_day = decimal_date(as_date(invoice_date))) %>%      
    group_by(customer_no, invoice_day) %>%
    summarise(daily_collections = sum(collection_amount)) %>% 
    mutate(invoice_dt = 
            round_date(date_decimal(invoice_day),
                       unit = "day" ) ) %>% 
  select(customer_no, invoice_date=invoice_dt, daily_collections)




# Next, we use the nest() function from the tidyr package to consolidate each time series by 
# group. The newly created list-column, “data.tbl”, contains the “order.month” and “total.qty” 
# columns by group from the previous step. The nest() function just bundles the data together 
# which is very useful for iterative functional programming.


daily_collections_nest <- 
  daily_collections_by_customer %>%
    group_by(customer_no) %>%
    nest()


### Step 1: Coerce to a ts object class

# In this step we map the tk_ts() function into a new column “data.ts”. The procedure is 
# performed using the combination of dplyr::mutate() and purrr::map(), which works really well 
# for the data science workflow where analyses are built progressively. As a result, this 
# combination will be used in many of the subsequent steps in this vignette as we build the 
# analysis.

#### mutate and map

# The mutate() function adds a column, and the map() function maps the contents of a list-column # (.x) to a function (.f). In our case, .x = data.tbl and .f = tk_ts. T


daily_collections_ts <- 
  daily_collections_nest %>%
    mutate(data.ts = map(.x       = data, 
                         .f       = tk_ts, 
                         select   = -invoice_date, 
                         start    = 2019,
                         freq     = 365))

## Step 2: Modeling a time series

# Next, we map the Exponential Smoothing ETS (Error, Trend, Seasonal) model function, ets, from 
#  the forecast package. Use the combination of mutate to add a column and map to interactively 
# apply a function rowwise to a list-column. In this instance, the function to map the ets 
# function and the list-column is “data.ts”. We rename the resultant column “fit.ets” indicating # an ETS model was fit to the time series data.


daily_collections_fit <- 
  daily_collections_ts %>%
    mutate(fit.ets = map(data.ts, ets))




### sw_tidy -- do some model inspection with the sweep tidiers.
# To get the model parameters for each nested list, we can combine sw_tidy within the mutate and map combo. 
# The only real difference is now we unnest the generated column (named “tidy”). 
# because it’s easier to compare the model parameters side by side, we add one additional call to spread() from the tidyr package.


daily_collections_fit %>%
    mutate(tidy = map(fit.ets, sw_tidy)) %>%
    unnest(tidy) %>%
    spread(key = customer_no, value = estimate)

### sw_glance -- view the model accuracies also by mapping sw_glance within the mutate and map combo.


daily_collections_fit %>%
    mutate(glance = map(fit.ets, sw_glance)) %>%
    unnest(glance)



### sw_augment


augment_fit_ets <- 
  daily_collections_fit %>%
    mutate(augment = map(fit.ets, sw_augment, timetk_idx = TRUE, rename_index = "date")) %>%
    unnest(augment)




# plot the residuals for the nine categories like so. Unfortunately we do see some very 

augment_fit_ets %>%
    ggplot(aes(x = date, y = .resid, group = customer_no)) +
    geom_hline(yintercept = 0, color = "grey40") +
    geom_line(color = palette_light()[[2]]) +
    geom_smooth(method = "loess") +
    labs(title = "A/R Collection Delay by Customer",
         subtitle = "ETS Model Residuals", x = "") + 
    theme_tq() +
    theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
    facet_wrap(~ customer_no, scale = "free_y", ncol = 3) 


### sw_tidy_decomp -- create decompositions using the same procedure with sw_tidy_decomp() and the mutate and map combo.

daily_collections_fit %>%
    mutate(decomp = map(fit.ets, sw_tidy_decomp, timetk_idx = TRUE, rename_index = "date")) %>%
    unnest(decomp)


## Step 3: Forecasting the model 


daily_collections_fcast <- 
  daily_collections_fit %>%
    mutate(fcast.ets = map(fit.ets, forecast, h = 12))

## Step 4: Tidy the forecast --  apply sw_sweep to get the forecast in a nice “tidy” data frame. #  use the argument fitted = FALSE to remove the fitted values from the forecast 


daily_collections_fcast_tidy <- 
  daily_collections_fcast %>%
    mutate(sweep = map(fcast.ets, sw_sweep, fitted = FALSE, timetk_idx = TRUE)) %>%
    unnest(sweep)


## Visualization 


daily_collections_fcast_tidy %>%
    ggplot(aes(x = index, y = daily_collections, color = key, group = customer_no)) +
    geom_ribbon(aes(ymin = lo.95, ymax = hi.95), 
                fill = "#D5DBFF", color = NA, size = 0) +
    geom_ribbon(aes(ymin = lo.80, ymax = hi.80, fill = key), 
                fill = "#596DD5", color = NA, size = 0, alpha = 0.8) +
    geom_line() +
    labs(title = "A/R Collection Delay by Customer",
         subtitle = "ETS Model Forecasts",
         x = "", y = "Units") +
    scale_color_tq() +
    scale_fill_tq() +
    facet_wrap(~ customer_no, scales = "free_y", ncol = 3) +
    theme_tq() +
    theme(axis.text.x = element_text(angle = 45, hjust = 1))

```


### Calculating Allowance for Uncollectable Accounts

Let the client's allowance for uncollectables accrual policy be:
  - 1\% of A/R under 45 days old are uncollectable
  - 2\% of A/R between 45 days and 120 days old are uncollectable
  - 5\% of A/R over 120 days old are uncollectable

The statistical estimation from the time-series analysis previously completed provides confidence bounds, for the forecast of optimistic, pessimistic and expected uncollectables.  This is 'overkill' for audits, and I will restrict the application of the client's accrual policy for just the expected rate of collection of receivables, based on prior experience with each customer.  Then we can apply policy, using forecast daily collections and outstanding A/R using the following code chunk.

```{r  error=F, warning=F, message=F, eval=F, fig.cap="Accrual Calculations for Allowance for Uncollectables (by customer)"}


library(knitr)
library(kableExtra)
library(tidyverse)
library(lubridate)

fyear_end <- paste0(format(Sys.Date(), '%Y'), '-12-31')   # set the fiscal year end date to match the dataset


## Set the directory for the new files (modify the path as needed)
default_dir <- "/home/westland/audit_analytics_book/audit_simulated_files/"

if (file.exists(default_dir)){
    setwd(default_dir)
} else {
    dir.create(default_dir)
    setwd(default_dir)
}


sales_journal <-  
  read.csv("sales_journal.csv", na.strings="0", stringsAsFactors=FALSE) 


credit_sales_journal <- 
  sales_journal %>% 
  filter(cash_not_ar == 0)



## calculate each customer's unpaid_ar

ar_by_customer <- 
  credit_sales_journal[
    credit_sales_journal$collection_date >= fyear_end & 
      credit_sales_journal$shipper_date <= fyear_end,] %>% 
  mutate(age = ymd(fyear_end) - ymd(invoice_date)) %>% 
  group_by(customer_no) %>% 
  summarize(sum(sales_extended),
            average_age = mean(age)) %>% 
  rename(customer_ye_balance='sum(sales_extended)')

  
## calculate each customer's forecasted collections
#(from the daily_collections_fcast_tidy data.frame) 
#including 80 and 95% confidence limits

forecast_by_customer <- 
daily_collections_fcast_tidy %>% 
  select(customer_no,key, daily_collections,lo.80,hi.80,lo.95,hi.95) %>% 
  filter(key == "forecast") %>% 
  group_by(customer_no) %>% 
  summarize(f_collections = mean(daily_collections),
            f_lo.80  = mean(lo.80),
            f_lo.95  = mean(lo.95),
            f_hi.80  = mean(hi.80),
            f_hi.95  = mean(hi.95)
            ) %>% 
  inner_join(ar_by_customer, by="customer_no") %>% 
  mutate(days_to_pay = customer_ye_balance / f_collections,
         age_when_fully_paid = days_to_pay + average_age)

## apply the client's allowance for uncollectables estimation
#policy assuming the steady daily rate of payment from the ETS

allowance_for_uncollectable_ar <- 
  forecast_by_customer %>% 
  mutate(age_when_fully_paid =
           as.numeric(age_when_fully_paid),
         uc_lt_45 = 
           ifelse(age_when_fully_paid < 
                    45, customer_ye_balance *  .01, 0),
         uc_45_120 =
           ifelse(age_when_fully_paid >=
                    45 & age_when_fully_paid <= 120,  
                  (1 - 45 / age_when_fully_paid) *
                    customer_ye_balance *  .02 +  
                    (45 / age_when_fully_paid) *
                    customer_ye_balance *  .01,  0),
         uc_gt_120 = 
           ifelse(age_when_fully_paid >
                    120,  
                 (1 - 120 / age_when_fully_paid) *
                   customer_ye_balance *  .05   + 
                   (75 / age_when_fully_paid) * customer_ye_balance * 
                   .02 + 45 * 
                   f_collections *  .01,  0),
         allowance = uc_lt_45 + uc_45_120 + uc_gt_120, 
                  pct_of_ye_balance = allowance / customer_ye_balance
           ) %>% 
  select(customer_no, allowance, pct_of_ye_balance)
         
total_allowance <- sum(allowance_for_uncollectable_ar$allowance)         
cat("Allowance for Uncollectable Accounts = ", total_allowance)

kable(allowance_for_uncollectable_ar, caption = "Accrual Calculations for Allowance for Uncollectables (by customer)","latex", booktabs = T) %>% 
  kable_styling(bootstrap_options = "striped")


```
