---
title: "ML_compliance_p235"
format: html
editor: visual
---


## Machine Learning Models for Audit of Controls

In this section I develop a deep learning model to assess total errors and misstatements in the population (all of the transactions for an audit year) based on the post-audit results of a sample of transactions.  This is a highly unbalanced dataset $-$ in most audits, the portion of transactions without an error will be several orders of magnitude larger than the number with errors. 


```{r eval=F, error=F, warning=F, message=F}


library(readr)
library(tidyverse)
library(compare)


## Set the directory for the new files (modify the path as needed)
default_dir <- "/home/westland/audit_analytics_book/audit_simulated_files/"

if (file.exists(default_dir)){
    setwd(default_dir)
} else {
    dir.create(default_dir)
    setwd(default_dir)
}


real_world_cash_sales <- 
  read.csv("real_world_cash_sales.csv", na.strings="0", stringsAsFactors=FALSE)

real_world_credit_sales  <- 
  read.csv("real_world_credit_sales.csv", na.strings="0", stringsAsFactors=FALSE)

sales_journal <-  
  read.csv("sales_journal.csv", na.strings="0", stringsAsFactors=FALSE) 

sales_journal[is.na(sales_journal)] <- 0

sales_journal <- split(sales_journal, sales_journal$cash_not_ar) 

credit_sales_journal <-  sales_journal$`0`
cash_sales_journal <- sales_journal$`1`


real_world_credit_sales$sales_extended <- real_world_credit_sales$sales_unit_price * 
  real_world_credit_sales$sales_count

credit_sales_journal$cost_extended <-
  credit_sales_journal$unit_cost *
  credit_sales_journal$sales_count

p1 <- hist(credit_sales_journal$sales_extended, breaks=20)
p2 <-hist(credit_sales_journal$cost_extended)

plot( p1, col=rgb(0,0,1,1/4), ylim=c(0,250), 
      main = "Cost (orange) vs. Revenue (purple) of Sales")  
plot( p2, col=rgb(1,0,0,1/4), add=T, ylim=c(0,250)) 


```


For an autoencoder to work well we need a strong initial assumption: that the distribution of variables for normal transactions is different from the distribution for ones that are in error.  A simple graphical example provides some insight into how the autoencoder can reveal control weaknesses at a systemic level.  In the following code chunk, I create a ridgeplot of column values for the `left_joined` files for sales on the client's books and actual sales, with a factor field for exceptions.  The ridgeplots highlight the errors in the client's sales records for `collection_date`, and `sales_unit_price`.  Ridgeplots are not particularly refined, but they can provide a quick method for 'eyeballing' systemic weaknesses.  


```{r  error=F, message=F, warning=F, fig.cap="Ridgeplots: Accounting Errors Change the Profile of the Transaction Data"}

library(tidyverse)
library(ggridges)


## Set the directory for the new files (modify the path as needed)
default_dir <- "/home/westland/audit_analytics_book/audit_simulated_files/"

if (file.exists(default_dir)){
    setwd(default_dir)
} else {
    dir.create(default_dir)
    setwd(default_dir)
}

real_world_sales <- 
  read.csv("real_world_credit_sales.csv",
           na.strings="0", stringsAsFactors=FALSE) %>% 
  select(invoice_no,
         sales_unit_price, 
         customer_no,
         sales_count,
         collection_amount,
         collection_date
         ) 

real_world_sales$sales_unit_price =
  real_world_sales$sales_unit_price*
  rbinom(nrow(real_world_sales),1,.7)

sales_journal <-
  read_csv("sales_journal.csv", 
   col_types = cols(X1 = col_skip()))%>% 
  filter(cash_not_ar == 0) %>% 
  select(invoice_no,
         sales_unit_price, 
         invoice_no,
         customer_no,
         sales_count,
         collection_amount,
         collection_date
         )

sales_except <- 
   left_join(sales_journal,
             real_world_sales, 
             by="invoice_no") 

if(sales_except$sales_unit_price.x != sales_except$sales_unit_price.y) {
  sales_except$exception = 1
} else {
  sales_except$exception = 0
}


sales_except %>% 
   select(exception,
    price_in_real_world = sales_unit_price.y,
    price_on_books = sales_unit_price.x,
    invoice_no,
    customer_no.y,
    sales_count.y,
    collection_amount.y,
    collection_date.y,
    customer_no.x,
    sales_count.x,
    collection_amount.x,
    collection_date.x
          ) %>%   
  gather(variable, value, -exception) %>%
  ggplot(aes(y = as.factor(variable), 
             fill = as.factor(exception), 
             x = percent_rank(value))) +
  geom_density_ridges()+
  theme(legend.position = "none")


 
```
The plots reveals the simple strategy used to alter `sales_unit_price` values, while other variables had no change in their distribution.   Real world situations will generally be more complex, but the basic concepts still apply, and this should be sufficient to demonstrate the autoencoder.


###  Autoencoders and Unbalanced Datasets 

An autoencoder is an unsupervised neural network that is used to learn a representation (encoding) for a set of data, typically for the purpose of dimensionality reduction. Since breaches should have a different distribution than non-breaches, we expect that our autoencoder will have higher reconstruction errors on breaches than on non-breaches. This means that we can use the reconstruction error as a quantity that indicates if a transaction is a breach or not.  The autoencoder has an hourglass shape, with a large number of trainable weights on input and output sides, with a bottleneck (only a few trainable parameters) in the middle.  

I constructed a simple deep-learning autoencoder with three fully-connected layers with $[100,10,100]$ weights at levels 1 through 3. Since errors should be distributed differently than non-error transactions, the autoencoder should exhibit higher reconstruction errors from the error dataset, and the the reconstruction error level can predict whether a transaction is in error. 

Compiling the model requires three choices: (1) a loss function, (2) an optimization algorithm, and (3) a performance metric (objective function).  
- Traditional parametric statistics use a squared error loss with  first-order calculus optimization and F or R-statistics measuring performance; these were easy to compute in a time before computers. 

- The current problem involves a binary choice (error or not-error); the most appropriate loss function is binary cross-entropy.  

- Optimization involves a complex response surface, and we choose the computationally intensive, but fast and efficient Adam optimizer [@kingma2014adam] for the task. 

- Mean-squared-error (MSE) provides straightforward performance metric for errors.

Real world data can vary widely in magnitude: an individual sale's quantity might be 2-3, but the inventory stock level for that same SKU might be 2000-3000 units.   Deep-learning algorithms work best when the data is presented at roughly comparable scales.  It is generally necessary to 'normalize' the data; otherwise convergence can be slow and unreliable. I split the sales dataset into train and test sets and then normalized the data to standardize on small input values of generally the same scale. Time data is also removed, as it is irrelevant for autoencoders (I discuss later the use of recurrent neural networks which are commonly used for time-series forecasting).  

I trained the model using Keras' `fit()` function, and used `callback_model_checkpoint()` in order to save the model after each epoch. By passing the argument `save_best_only = TRUE` I save only the epoch (i.e., pass of the dataset) with smallest loss value on the test set. I also use callback_early_stopping() to stop training if the validation loss stops decreasing for 15 epochs (i.e., passes of the dataset).


```{r echo=F, eval=T, message=F, warning=F}

## load Keras, etc.  You need to have installed Tensorflow for this to work
library(tidyverse)
library(keras)
install_keras()


```

R will restart after the installation of keras.  We can then proceed to build our deep-learning model.


```{r  error=F, message=F, warning=F, fig.cap="Machine Learning"}

library(tidyverse)
library(lubridate)
library(stringr)
library(keras)


use_session_with_seed(123) ## make sure results are reproducible
## The use_session_with_seed() function 
## establishes a common random seed for R, Python, NumPy, and TensorFlow. 
## It furthermore disables hash randomization, GPU computations, and CPU parallelization, 
## which can be additional sources of non-reproducibility.  


## Set the directory for the new files (modify the path as needed)
default_dir <- "/home/westland/audit_analytics_book/audit_simulated_files/"

if (file.exists(default_dir)){
    setwd(default_dir)
} else {
    dir.create(default_dir)
    setwd(default_dir)
}

real_world_sales <- 
  read.csv("real_world_credit_sales.csv",
           na.strings="0", stringsAsFactors=FALSE) %>% 
  select(invoice_no,
         sales_unit_price, 
         customer_no,
         sales_count,
         collection_amount,
         collection_date
         ) 

real_world_sales$sales_unit_price  <- 
  real_world_sales$sales_unit_price*
  rbinom(nrow(real_world_sales),1,.9)

sales <-
  read_csv("sales_journal.csv", 
   col_types = cols(X1 = col_skip()))%>% 
  filter(cash_not_ar == 0) %>% 
  select(invoice_no,
         sales_unit_price, 
         invoice_no,
         customer_no,
         sales_count,
         collection_amount,
         collection_date
         )


sales_w_error <- 
   left_join(sales,
             real_world_sales, 
             by="invoice_no") 

if(sales_except$sales_unit_price.x !=
   sales_except$sales_unit_price.y) {
  sales_w_error$error = TRUE
} else {
  sales_w_error$error = FALSE
}

sales_w_error[is.na(sales_w_error)] <- 0

sales_w_error$collection_date.x <-
  as.numeric(
    decimal_date(as.POSIXct(sales_w_error$collection_date.x)))

sales_w_error$collection_date.y <-
  as.numeric(decimal_date(
    as.POSIXct(sales_w_error$collection_date.y)))


y <- sales_w_error %>%  select (error)

x <- sales_w_error %>% 
  select (price = sales_unit_price.x, 
          count = sales_count.x,
          amt_coll = collection_amount.x,
          date_coll = collection_date.x,
          real_price = sales_unit_price.y,
          real_count = sales_count.y,
          real_amt_coll = collection_amount.y,
          real_date_coll =collection_date.y,
          error) 

c <- x$error

freq <- 100 * nrow(x[x$error!=0,])/nrow(x[x$error==0,])
cat("Error frequency = ", freq, "% ")

bucket <- sample(2, nrow(sales), replace=T, prob = c(0.75, 0.25))  # 3:1 split

y_train <- y[bucket==1,]
y_test <- y[bucket==2,]
 
# don't use the error for ML 
# the auditor doesn't know the errors, 
# these are something to be found during the audit)
x_train <- x[bucket==1,1:8] 
x_test <- x[bucket==2,1:8]
x_train_w_err <- x[bucket==1,9]  # save error for later
x_test_w_err <- x[bucket==2,9]

col_means_train <-as.numeric(apply(x_train,2,mean))   ## invoice is an ID           
col_stddevs_train <- as.numeric(apply(x_train,2,sd))            

# Normalize training data
x_train <-  as.matrix(scale(x_train, center = col_means_train, scale = col_stddevs_train))


# Test data is *not* used when calculating the mean and std.
x_test <-  as.matrix(scale(x_test, center = col_means_train, scale = col_stddevs_train))



## The autoencoder model

x_train_model <- as.matrix(x_train)  ## don't use the error to train

model <- keras_model_sequential()
model %>%
  layer_dense(units = 100, activation = "tanh", input_shape = ncol(x_train)) %>%
  layer_dense(units = 10, activation = "tanh") %>%
  layer_dense(units = 100, activation = "tanh") %>%
  layer_dense(units = ncol(x_train))

summary(model)


# Compile and fit the model to the data  
## Use callback_model_checkpoint() 
## in order to save the model after each epoch.
## By passing the argument save_best_only = TRUE 
## I save only the epoch (i.e., pass of the dataset) 
## with smallest loss value on the test set.


model %>% compile(
   loss = "binary_crossentropy", 
   optimizer = "adam",
   metrics= 'mse'
)

checkpoint <- callback_model_checkpoint(
  filepath = "model.hdf5",      ## Keras model weights are saved to HDF5 format. 
  save_best_only = TRUE, 
  period = 10,
  verbose = 1
)

early_stopping <- callback_early_stopping(patience = 10)

history <- 
  model %>% 
  fit(
  x = x_train[y_train == 0,], 
  y = x_train[y_train == 0,], 
  epochs = 50,        ## potentially complex model so train for a long time
  batch_size = 8192,   ## adjust for your memory size
  validation_data = list(x_test[y_test == 0,], x_test[y_test == 0,]),
  callbacks = list(checkpoint, early_stopping)
)
  
plot(history)

# After training we can get the final loss for the test set by using the evaluate() function.


loss <- evaluate(model, x = x_test[y_test == 0,], y = x_test[y_test == 0,])
loss


```

Once the model is trained and tuned the autoencoder can be used to generate predictions.  We assumed that the the mean-squared-error (MSE) of error transactions will be higher, and this can be used to predict errors in unaudited populations.  

A good measure of model performance in highly unbalanced datasets is the Area Under the ROC Curve (AUC). AUC has a straightforward interpretation for the audit task; it is the probability that an error transaction will have higher MSE then a non-error transaction. Calculate this using R's *Metrics* package, which implements a wide variety of common machine learning model performance metrics.


```{r echo=T, eval=T, message=F, warning=F}

pred_train <- predict(model, x_train)
mse_train <- apply((x_train - pred_train)^2, 1, sum)

pred_test <- predict(model, x_test)
mse_test <- apply((x_test - pred_test)^2, 1, sum)


library(Metrics)
train_acc <- auc(y_train, mse_train)
test_acc <- auc(y_test, mse_test)
train_acc 
test_acc 

```

The values returned by the `Metrics::auc` function give the probability that an error transaction will have higher MSE than a non-error transaction.  From the above analysis, the probability that errors have higher MSE is quite high, and the autoencoder is able to generate very informative predictors for the sales transaction data.  Making predictions involves finding a threshold $k$ such that if $MSE > k$ the auditor decides that the transaction is in error.  To define this value it is useful to look at the information science measures of precision and recall while varying the threshold $k$.  The following code creates two plots that do just that, using an abbreviated form of R's `ggplot` called `qplot`, mapping precision and recall against the decision threshold for deciding whether or not to audit a transaction. 

```{r echo=T, eval=F, message=F, warning=F, fig.cap="Precision and Recall at Various MSE Thresholds"}


possible_k <- seq(0, 0.5, length.out = 100)      # range of threshold values k
precision <- sapply(possible_k, function(k) {    # create a vector of precision x k
  predicted_class <- as.numeric(mse_test > k)
  sum(predicted_class == 1 & y_test == 1)/sum(predicted_class)
})

qplot(possible_k, precision, geom = "line") + labs(x = "Decision threshold k", y = "Precision")


recall <- sapply(possible_k, function(k) {     # create a vector of recall x k
  predicted_class <- as.numeric(mse_test > k)
  sum(predicted_class == 1 & y_test == 1)/sum(y_test)
})
qplot(possible_k, recall, geom = "line")  + labs(x = "Decision threshold k", y = "Recall")

```

These two graphs provide insight into the trade-offs we make in threshold choice, but they are not directly interpretable into specific choices in the audit.  For that, it is necessary to map the trade-off between: 

1. audit cost, which will depend on sample size $n$, and 
2. audit risk, i.e., the probability of accepting an account value $\alpha$ (in this case sales) containing an intolerable error.  This in turn depends on the auditor's choice of intolerable error level $M$

The auditor starts with the assumption that the transaction system is 'out-of-control' $X\%$ of the years, based on prior years' workpapers. In the current year the system is by default assumed to be 'in-control'.  Use a one-sample proportion test to see if the probability of an 'out-of-control' transaction system is significantly different from zero.  Significance is measured by p-value; if this falls below the significance threshold, say 0.05, we can conclude that the transaction processing is 'out-of-control'.  This can be determined using R's `pwr.p.test` function in the `pwr` package.

Note that the thresholds for the following tests are for MSE of the model predictions, and will be different from those of precision and recall (which are simply ratios of counts). The prediction process identifies errors train_acc = ~87% of time  thus sampling from these predictions can be thought of as a 'distilled' sample of errors that are $\frac{1}{1-train_acc}$ as concentrated as in a sample from all of the population.  The estimate of population mean from the prediction sample multiplies the sample estimate of error by $\frac {(population-size) * (1-train_acc)}{sample-size}$  for a 95% confidence (5\% significance) this will be $z_{.95} \approx 1.96$

```{r echo=T, eval=F, message=F, warning=F, results='hold' , tidy=T, error=F}

library(tidyverse)


# range of sample sizes
r <- seq(10,100,10)
nr <- length(r)

# thresholds
k <-  seq(.5,20,.5)
nk <- length(k)

pred_mean <- array(numeric(nr*nk), dim=c(nr,nk))
full_mean <- array(numeric(nr*nk), dim=c(nr,nk))
pred_sd <- array(numeric(nr*nk), dim=c(nr,nk))
full_sd <- array(numeric(nr*nk), dim=c(nr,nk))

pop_error_pred <- array(numeric(nr*nk), dim=c(nr,nk))
pop_error_full <- array(numeric(nr*nk), dim=c(nr,nk))




## recover the error in trainiing and test data to do sampling
x_train <- cbind(x_train,x_train_w_err)
x_test <- cbind(x_test,x_test_w_err)
x_test <- cbind(x_test,mse_test)



for(l in 1:nk) {

  predicted <- x_test[mse_test > k[l],]

  for (j in 1:nr){


pred_sample <- dplyr::sample_n(as_tibble(predicted), r[j], replace=T)  

## sample_n is part of tidyverse
## a tibble is tidyverse's version of a data.frame 
## (a cute variation on 'table')


pred_mean[j,l] <- mean(pred_sample$x_test_w_err)
pred_sd[j,l] <- sd(pred_sample$x_test_w_err)

## for a 95% confidence (5% significance) this will be $z_{.95} \approx 1.96$

pop_error_pred[j,l] <- 
  (pred_mean[j,l] + pred_sd[j,l] * 1.96) *   ## one-sided 95% CL
  nrow(sales)*(1-train_acc)/r[j]  


## sample from all rows and compute sample mean-sd

full_sample <- dplyr::sample_n(as_tibble(x_test), r[j])    
full_mean[j,l] <- mean(full_sample$x_test_w_err)
full_sd[j,l] <- sd(full_sample$x_test_w_err)    
pop_error_full[j,l] <- 
  (pred_mean[j,l] + pred_sd[j,l] * 1.96) * 
  nrow(sales) / r[j]

  }
}

## save all of our computed data for the sales transaction audit; 
# reload with load("audit_data.RData") 
save(
  pop_error_full, 
  pop_error_pred,  
  pred_mean, 
  pred_sd, 
  full_mean, 
  full_sd, 
  file="audit_data.RData")



```



```{r eval=F, error=F, echo=T, results="hide", message=F, fig.cap="Deep-learning generates tighter bounds on Population Error Estimates"}

library(dplyr)
library(plotly)
library(reshape)
library(ggplot2)
library(scales)   # needed for formatting y-axis labels to non-scientific type

# webshot::install_phantomjs()  # phantomjs is required to run plotly, install webshot first

r <- seq(10,100,10)
nr <- length(r)

k <- seq(.5,20,.5)
nk <- length(k)


pred_scatter <- melt(pop_error_pred)
colnames(pred_scatter)=c("sample_size", "threshold", "error_est")
full_scatter <- melt(pop_error_full)
colnames(full_scatter)=c("sample_size", "threshold", "error_est")

full_scatter$sample_size <- full_scatter$sample_size * 20 
full_scatter$threshold <- full_scatter$threshold * .5 

pred_scatter$sample_size <- pred_scatter$sample_size * 20 
pred_scatter$threshold <- pred_scatter$threshold * .5 

pred_scatter$pred_or_full <- "predicted"   ## indicator for plotting
full_scatter$pred_or_full <- "full"

full_pred_scatter <- rbind(full_scatter, pred_scatter)
grouped <- full_pred_scatter %>% dplyr::group_by(sample_size,pred_or_full)
full_pred_2D_plot <- summarize(grouped, mean=mean(error_est))


p <- full_pred_2D_plot %>% 
  ggplot(aes(sample_size, mean, color=pred_or_full)) + 
  geom_smooth(se=F) +
  xlab("Sample Size") +
  ylab("Estimated Population Error") +
  scale_y_continuous(labels = comma) 

p

q <- 
  pred_scatter %>% 
  plot_ly(
    x = ~sample_size, 
    y = ~threshold, 
    z = ~error_est) %>%
  add_markers(size=10) %>%
  layout(scene = list(xaxis = list(title = 'sample size'),
                     yaxis = list(title = 'MSE decision threshold'),
                     zaxis = list(title = 'estimated error in sales account')))

#  type "q" if you would like to generate 
# an interactive 3D graph of 
# the predicted population errors



```


In this example, using the deep-learning autoencoder to create error rich samples provided the auditor the ability, for 95% confidence (5% significance) level of audit risk, to estimate much tighter bounds for the population error for any sample size.  This is possible, because the deep-learning 'distillation' process solves the problem of highly unbalanced error data $-$ i.e., that there are many more correct transactions than errors.  




```{r error=F, echo=F, out.width="100%", fig.cap="Sample Size and Optimal MSE Threshold Values 'k' for Prediction of Error Rich Datasets"}


knitr::include_graphics(
  "/home/westland/audit_analytics_book/aaa_chapters/pictures/plotly_just_predictions.png", 
    dpi = 300)


```

