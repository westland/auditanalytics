---
title: "web_scraping_p166"
format: html
editor: visual
---

## Accessing General Web Content through Web Scraping

Where relevant intelligence is not available through APIs, but is presented on websites, it is possible to *web scrape* data.  This can be difficult and messy, but R provides a number of very effective helper tools to scrape and organize data from websites.  I provide here a brief introduction to the concept and practices of web scraping in R using the `rvest` package. Tools like `rvest` and *Beautiful Soup* (Python) inject structure into web scraping, which has become important because so few companies are willing to part with their proprietary customer data sets.  They have no choice but to expose some of this proprietary data via the web, though, and this is where auditors have an opportunity to accumulate valuable information germane to audit risk. The process of scraping data from the web exemplifies the computer-plus-human model of computing. It is also a nice introduction to building custom software for scraping a specific website.

The basic functions in `rvest` are powerful, and you should try to utilize the following functions when starting out a new webscrapping project.

- `html_nodes()`: identifies HTML wrappers.
- `html_nodes(".class")`: calls node based on css class
- `html_nodes("#id")`: calls node based on <div> id
- `html_nodes(xpath="xpath")`: calls node based on xpath 
- `html_attrs()`: identifies attributes (useful for debugging)
- `html_table()`: turns HTML tables into data frames
- `html_text()`: strips the HTML tags and extracts only the text
     
Note on plurals: `html_node()` returns metadata; but `html_nodes()` iterates over the matching nodes. The `html_nodes()` function turns each HTML tag into a row in an R dataframe.


### SelectorGadget

*SelectorGadget* is a *javascript bookmarklet* that allows you to interactively figure out what *css selector* you need to extract desired components from a page. To install it, go to the page:

    https://cran.r-project.org/web/packages/rvest/vignettes/selectorgadget.html

Install *selectorgadget* on the Chrome Browser (only at the time of this writing) from  https://selectorgadget.com/.   SelectorGadget is an open source tool that simplifies CSS selector generation and discovery on complicated sites.   Install the Chrome Extension or drag the bookmarklet to your bookmark bar, then go to any page and launch it. A box will open in the bottom right of the website. Click on a page element that you would like your selector to match (it will turn green). *SelectorGadget* will then generate a minimal CSS selector for that element, and will highlight (yellow) everything that is matched by the selector. Now click on a highlighted element to remove it from the selector (red), or click on an unhighlighted element to add it to the selector. Through this process of selection and rejection, *SelectorGadget* helps you come up with the perfect CSS selector for your needs. 

To use it, open the page:

1. Click on the element you want to select. *Selectorgadget* will make a first guess at what css selector you want. It’s likely to be bad since it only has one example to learn from, but it’s a start. Elements that match the selector will be highlighted in yellow.

2. Click on elements that should not’t be selected. They will turn red. Click on elements that should be selected. They will turn green.

3. Iterate until only the elements you want are selected. Selectorgadget is not perfect and sometimes won’t be able to find a useful css selector. Sometimes starting from a different element helps.

Other important functions: 

1. If you prefer, you can use xpath selectors instead of css: `html_nodes(doc, xpath = "//table//td"))`.

2. Extract the tag names with `html_tag()`, text with `html_text()`, a single attribute with `html_attr()` or all attributes with `html_attrs()`.

3. Detect and repair text encoding problems with `guess_encoding()` and `repair_encoding()`.

4. Navigate around a website as if you’re in a browser with `html_session()`, `jump_to()`, `follow_link()`, `back()`, and `forward()`. Extract, modify and submit forms with `html_form()`, `set_values()` and `submit_form()`.  


#### Example: Simple sentiment analysis 

Here is an example of a simple sentiment analysis for customers comments on restaurants in Hanoi Vietnam.  Start by pointing your browser to https://www.tripadvisor.com and searching for "Asian" cuisine in "Hanoi"  (as homework, consider other cities or services based on specific audit needs).  Click on the "Asian" menu, which brings you to web page https://www.tripadvisor.com/Restaurants-g293924-Hanoi.html.  Turn on  *selectorgadget* in Chrome browser and highlight all of the reviews.  In the menu at the bottom of your screen, this will give you an index `".is-9"` which is the designator for the CSS code that you have outlined (you can verify this in Chrome by clicking the three dot menu at the upper right-hand corner of the screen, clicking "More Tools" = "Developer Tools" and checking the webpage HTML; or right-click and inspect for a quick look)

```{r message=F,error=F, warning=F}


library(rvest)
library(RColorBrewer)
library(wordcloud)
                 
                 ## Copy the URL of the page you are scraping
                 url <- "https://www.tripadvisor.com/Restaurants-g293924-Hanoi.html"
                 
## Extract the reviews in the CSS ".is-9" selector
reviews <- url %>%
    read_html() %>%
    html_nodes(".is-9")
                 
## Pull the text out of the reviews
quote <- reviews %>%  html_text()
                 
                 ## Turn the character string "quote" into a data.frame and View
 data.frame(quote, stringsAsFactors = FALSE) %>% View()
                 
pal2 <- brewer.pal(8,"Dark2") ## from RColorBrewer
                 
wordcloud(quote, colors=pal2)

```

In TripAdvisor, you can use the same methods, in various geographical regions, for: Hotels, Things to do, Restaurants, Flights, Vacation Rentals,Cruises and other things.  Similar methods work for other review and aggregation sites.


#### Example: Movie Reviews 

The next example scrapes information about The Lego Movie from IMDB. We start by downloading and parsing the file with `html()`.  To extract the rating, we start with *Selectorgadget* to figure out which css selector matches the data we want.  We use `html_node()` to find the first node that matches that selector, extract its contents with `html_text()`, and convert it to numeric with `as.numeric()`.

```{r message=F,error=F, warning=F}

library(rvest)
lego_movie <- html("http://www.imdb.com/title/tt1490017/")

lego_movie %>%
  html_node("strong span") %>%
  html_text() %>%
  as.numeric()

## We use a similar process to extract the cast, 
## using html_nodes() to find all nodes that match the selector:

lego_movie %>%
  html_nodes("#titleCast .itemprop span") %>%
  html_text()

``` 


Next find the actors listed on "The Lego Movie" IMDB movie page:

1. Navigate to the page and scroll to the actors list.  
2. Click on the selectorgagdget link in the bookmarks. The selectorgadget console will appear at the bottom of the screen, and element currently under the mouse will be highlighted in orange.
3. Click on the element you want to select (the name of an actor). The element you selected will be highlighted in green. Selectorgadget guesses which css selector you want (.itemprop in this case), and highlights all matches in yellow.
4. Scroll around the document to find elements that you don’t want to match and click on them. For example, we don’t to match the title of the movie, so we click on it and it turns red. The css selector updates to `#titleCast .itemprop`.


```{r message=F,error=F, warning=F}

library(rvest)

html <- read_html("http://www.imdb.com/title/tt1490017/")
cast <- html_nodes(html, "#titleCast .itemprop")
length(cast)

cast[1:2]

## Looking carefully at this output, we see twice as many matches as we expected. 
## That’s because we’ve selected both the table cell and the text inside the cell. 
## We can experiment with selectorgadget to find a better match or look at the html directly.

cast <- html_nodes(html, "#titleCast span.itemprop")
length(cast)


html_text(cast)

```


#### Example: Tabular Data

Some websites publish their data in an easy-to-read table without offering the option to download the data. Package `rvest` uses `html_table()` for tabular data. Using the functions listed above, isolate the table on the page.  Then pass the HTML table to html_table(). In the following case, you can go to https://www.nis.gov.kh/cpi/ and inspect the html.

```{r}

library(rvest)
library(tidyverse)

accounts <- read_html("https://www.nis.gov.kh/cpi/Apr14.html") 

table <- accounts %>%
  html_nodes("table") %>%
  html_table(header=T)

# lean up the table  
# table[[1]]
dict <- table[[1]][,1:2]
accounts_df <- table[[1]][6:18,-1]

names <- c('id', 'weight.pct', 'jan.2013', 'dec.2013', 'jan.2014', 'mo.pctch', 'yr.pctch', 'mo.cont', 'yr.cont')
colnames(accounts_df) <- names

glimpse(accounts_df)


```


#### Example: XPaths

Xpaths are content hierarchies in a website.  Sometimes you can get more comprehensive retrieval with an xPath. You can get the xpath that includes some content with the Chrome *xPath Finder* extension (it's like *SelectorGadget* but for xPaths)


```{r echo=T,message=F,error=F, warning=F}

# example of scraping a table with an XPath
library(rvest)
library(tidyverse)

h <- read_html(
  "https://en.wikipedia.org/wiki/Current_members_of_the_United_States_House_of_Representatives")

reps <- h %>% html_node(xpath = '//*[@id="votingmembers"]') %>%
 html_table(fill=T)
reps <- reps[,c(1:2,4:9)] %>% as_tibble()

```


#### Example: Extracting Intelligence from Product User Forums

Product user forums are excellent sources of informed consumer and retailer information.  This example provides a number of methods that can be used for general web scraping.  Applying this to our  goal of web scraping for intelligence on Roland's products, we can glean consumer sentiment on Roland's pianos as conveyed by discussions on the Piano World Forum website and display it with `wordcloud`.


```{r eval=T,message=F,error=F, warning=F}

#install.packages(c("tm", "SnowballC", "wordcloud", "RColorBrewer", "RCurl", "XML"))

library(tm)
library(SnowballC)
library(RCurl)
library(tidyverse)
library(rvest)
library(RColorBrewer)
library(wordcloud)
library(stringr)
library(httr)
library(XML)

handle <- handle("http://forum.pianoworld.com//") 
path   <- "ubbthreads.php/ubb/login.html?ocu=http%3A%2F%2Fforum.pianoworld.com%2F"

# fields found in the login form.
login <- list(
  amember_login = "westland"
 ,amember_pass  = "powerpcc"
 ,amember_redirect_url = 
   "http://forum.pianoworld.com//ubbthreads.php/forum_summary.html"
)

response <- POST(handle = handle, path = path, body = login)


                 
# Copy the URL of the page you are scraping
url <- "http://forum.pianoworld.com/"
               
# Extract the reviews in the CSS selector
reviews <- url %>%
           read_html() %>%
           html_nodes("#cat2 div") 
             
# Pull the selected text out of the reviews
quote <- reviews %>%
                  html_text() %>% 
                   as.tibble()

quote <-  filter(quote[str_detect(quote, "Roland")])

pal2 <- brewer.pal(8,"Dark2") # from RColorBrewer

wordcloud(unlist(quote), colors=pal2)

```


