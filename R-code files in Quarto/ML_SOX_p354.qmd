---
title: "ML_SOX_p354"
format: html
editor: visual
---

##  Using an Autoencoder to Detect Control Weaknesses

I designed two autoencoders, one in `Tensorflow / Keras` for reference, and one in `H2O` for analysis of SOX control weaknesses, `H2O` (https://www.h2o.ai/) is a popular open source machine learning platform accessible in R which runs both in-memory and on `H2O` servers, offering an interface that is slightly simpler to use than Tensorflow's.  `H2O` supports the most widely used statistical & machine learning algorithms including gradient boosted machines, generalized linear models, deep learning and so forth, and contains an _AutoML_ function that automatically runs through all the algorithms and their hyperparameters to produce a leaderboard of the best models.  For larger models, `H2O4GPU` provides GPU-accelerated machine learning on hardware platforms with, e.g., NVidia's GPUs.


As is typical with accounting data, predictors are multicolinear, which creates problems for traditional regression approaches because of the singularities.  Figure 5's cursory plot of principal components (a linear clustering algorithm) reveals that most of the variance is weighted on the first principal component.   This does not mean that the data doesn't convey information; it's just that the information involves more complex multicovariate non-linear relationships.  Because of this, machine learning methods that train many weights in multiple layers are likely to extract this information more effectively.  This single component is likely to be a surrogate for 'control weakness', suggesting covariance between SOX reports and occurrence of security breaches (otherwise, we would likely see clustering around two large principle components).


```{r  echo=F, output.width="50%", warning=F, error=F, message=F,         fig.cap="Eigenvalues of the PCA Decomposition"}

scree_sox <- read.csv("/home/westland/Documents/SOX-AI/innerjoinSOXBRCH.csv")
scree_sox <- scree_sox[,-c(1:2)]
scree_sox[is.na(scree_sox)] <- 0
screeplot(prcomp(scree_sox), 
          main="The \"Control Weakness\" Principle Component ", 
          xlab="Component"
          )

```

Figure 5 shows the multicolinearity of the combined SOX and Privacy Clearinghouse information -- most of the information in both datasets can be captured in one principle component.  Given that SOX reports control weaknesses, and security breaches occur because of control weaknesses, this principle component is a strong surrogate for formal definitions of "control weakness".



## Preprocessing

Preprocessing splits the dataset into train and test sets and then *min-max* normalize the data (this is done because neural networks work much better with small input values). Based on the `date` variable, we will use the results before 2011 for training and the rest for testing. This is good practice because when using the model we want to predict future breaches based on SOX reports issued previously.

Two helper functions are used to normalize the database (i.e., adjust the values to similar scale, which helps the machine learning algorithms converge more quickly).  The are not really needed for this data, which is binary $[0,1]$ data, but if you wish to reuse this code on other data, they will be useful.  The helper functions include:

1. a function to get descriptive statistics about the dataset that are used for min-max scaling (using the same normalization constants for training and test sets), and
1. a function to perform the min-max scaling. 

```{r eval=T, warning=F, error=F, message=F}

library(tidyverse)
library(purrr)
library(keras)

## function to get descriptive statistics 

get_desc <- function(x) {
  map(x, ~list(
    min = min(.x),
    max = max(.x),
    mean = mean(.x),
    sd = sd(.x)
  ))
} 

## function to normalize values

normalization_minmax <- 
  function(x, desc) {
  map2_dfc(
    x, 
    desc,
    ~(.x - .y$min)/(.y$max - .y$min))
}


## get data
sox <-
  read_csv(
    "~/Documents/SOX-AI/innerjoinSOXBRCH.csv", 
    col_types = cols(X1 = col_skip())
    ) %>% 
  select(
    date,
    CARD.x,            
    IS_EFFECTIVE,     
    MATERIAL_WEAKNESS, 
    SIG_DEFICIENCY,    
    IC_OP_TYPE,       
    AUDITOR_AGREES,    
    COMBINED_IC_OP,   
    IC_IS_EFFECTIVE, 
    AUDIT_FEES
  ) 
sox[is.na(sox)] <- 0

## create normalized datasets for ML

desc <- sox %>% get_desc()

x_train <-  
  sox %>% 
  filter(date <= 2011) %>%     
  normalization_minmax(desc)  %>% 
  select(-c(date))

x_test <- 
  sox %>% 
  filter(date > 2011) %>% 
  normalization_minmax(desc)  %>% 
  select(-c(date)) 

y_train <- 
  x_train[,"CARD.x"] %>%   ## CARD.x the fraud indicator
  as.matrix()

y_test <- 
  x_test[,"CARD.x"] %>% 
  as.matrix()
  
x_train <- 
  x_train %>% 
  select(-c(CARD.x)) %>%
  as.matrix()

x_test <- 
  x_test %>% 
  select(-c(CARD.x)) %>%
  as.matrix()

```



## Tensorflow Implementation of the Autoencoder

Here is the reference autoencoder in `Tensorflow / Keras`.  The model converges quickly and accurately, suggesting that SOX control weakness reporting can be a successful predictor of vulnerability to credit card fraud at client firms.

```{r}
## assure that the Keras API is available

library(keras)
install_keras()

```



```{r fig.cap="Convergence of Symmetric Autoencoder with 4 fully-connected Layers in Detecting Fraud with SOX Data"}

## define the model in Keras, 
## this is a symmetric autoencoder with 4 fully connected layers.


model <- keras_model_sequential()
model %>%
  layer_dense(units = 100, activation = "tanh", input_shape = ncol(x_train)) %>%
  layer_dense(units = 10, activation = "tanh") %>%
  layer_dense(units = 100, activation = "tanh") %>%
  layer_dense(units = ncol(x_train))

summary(model)


## the model is a binary classifier, so we use an entropy loss 

model %>% compile(
  loss = "binary_crossentropy",  
  optimizer = "adam",
  metrics= 'accuracy'
)

checkpoint <- callback_model_checkpoint(
  filepath = "model.hdf5", 
  save_best_only = TRUE, 
  period = 1,
  verbose = 1
)

early_stopping <- callback_early_stopping(patience = 5)

history <- model %>% fit(
  x = x_train[y_train == 0,], 
  y = x_train[y_train == 0,], 
  epochs = 10, 
  batch_size = 32,
  validation_data = list(x_test[y_test == 0,], x_test[y_test == 0,]), 
  callbacks = list(checkpoint  , early_stopping)
)

plot(history)

```






## The H2O implementation of autoencoders and anomaly detection for fraud analytics

The _h2o_ package (https://www.h2o.ai/) provides the R interface to the H2O open source machine learning platform which supports gradient boosted machines, generalized linear models, deep learning and models. It has become popular due to *AutoML* which runs through algorithms and their hyperparameters to produce a leaderboard of the best models.  It works on a wide range of big data infrastructures, Hadoop or Spark clusters and can directly read files from HDFS, Spark, S3, Azure Data Lake and a variety of other data sources into it’s in-memory distributed key value store, and is relatively easy to productize.  R also offers a GPU-accelerated H2O package _h2o4gpu_ package.  The following example applies H2O to industry wide security assessment using SOX data.  A more extensive implementation of stochastic grid hyperparamter search for optimal fit can be found at [@westland2020cardfraud].

The SOX-card fraud dataset needs special treatment when performing machine learning because they are severely unbalanced,  Only ~$3.44 \% $ of audits were for companies that experienced a credit card fraud, affirming that the database is highly unbalanced.

```{r}

cat("% of SOX reports where client had credit fraud during the year = ", 100*sum(sox$CARD.x)/length(sox$CARD.x), "% of audits")

```
In such cases, the optimal predictor would predict in $100 \%$ of audits that there would be no credit card frauds, and that predictor would be right a respectable $96.56 \%$ of the time.  A closer look at audit fee distributions and dates of frauds suggests that fraud occurrence is not random, and suggests there is information to be extracted from the dataset.


```{r message=F, error=F, warning=F, fig.cap="Count Profiles of Fraud and Non-Fraud Cases of Audit Fees and Reporting Dates"}

library(tidyverse)

sox %>%
  ggplot(aes(x = AUDIT_FEES)) +
    geom_bar(color = "grey", fill = "lightgrey") +
    theme_bw() +
    scale_y_continuous(trans='log2') +
    facet_wrap( ~ CARD.x, scales = "free", ncol = 2)


sox %>%
  ggplot(aes(x = date)) +
    geom_bar(color = "grey", fill = "lightgrey") +
    theme_bw() +
    facet_wrap( ~ CARD.x, scales = "free", ncol = 2)
```

Interestingly, fraudulent credit card transactions had much more variability in the audit fees charged.  This could have many causes, but it also seems to suggest that where fraud is a possibility, there may be greater uncertainty in the client's controls that necessitates additional audit steps.


For modeling, I used R’s `H2O` implementation and converted the dataset to `H2O` format, splitting the dataset into training and test sets.

```{r}

library(h2o)
h2o.init(nthreads = -1)

creditcard_hf <- as.h2o(sox)

splits <- h2o.splitFrame(creditcard_hf, 
                         ratios = c(0.30, 0.30), 
                         seed = 123)

train_unsupervised  <- splits[[1]]
train_supervised  <- splits[[2]]
test <- splits[[3]]

response <- "CARD.x"
features <- setdiff(colnames(train_unsupervised), response)

```
In `H2O` I start by training an unsupervised autoencoder neural network by setting `autoencoder = TRUE`.  Similar to the `Tensorflow` model, I use a bottleneck model that reduces the dimensionality of the data down to 2 nodes/dimensions.  The autoencoder then + learns which credit card transactions are similar and which transactions are outliers or anomalies.  Autoencoders can be sensitive to outliers, and thus may be prone to overfitting.


```{r}
library(h2o)

model_nn <- h2o.deeplearning(x = features,
                             training_frame = train_unsupervised,
                             model_id = "model_nn",
                             autoencoder = TRUE,
                             reproducible = TRUE, 
                             seed = 123,
                             hidden = c(10, 2, 10), 
                             epochs = 100,
                             activation = "Tanh")

model_nn
#Convert to autoencoded representation
test_autoenc <- h2o.predict(model_nn, test)

```

Bottleneck autoencoders offer the ability to extract features captured in the middle layers, similar to factor analysis or PCA, but the features need not be linear nor must they adhere to the simple optimization of traditional exploratory statistics. . We can extract hidden features with the h2o.deepfeatures() function and plot to show the reduced representation of the input data.


```{r fig.cap="Autoencoder Layer-2 Detection of Fraud Cases with SOX Data"}

library(tidyverse)

train_features <- h2o.deepfeatures(
  model_nn, 
  train_unsupervised, 
  layer = 2
  ) %>%
  as.data.frame() %>%
  mutate(card_fraud = as.vector(train_unsupervised[, "CARD.x"]))

ggplot(
  train_features, 
  aes(x = DF.L2.C1, y = DF.L2.C2, color = card_fraud)) +
  geom_point(alpha = 0.1)

```

Graphing the two features extracted from layer 2 of the autoencoder shows that feature `DF.L2.C1` clearly dichotomizes audits into those with high potential for card fraud, and those without.  Dimensionality reduction through an autoencoder model alone can clearly identify fraud in this dataset.  This concept can be carried forward further, using the reduced dimensionality representation of one of the hidden layers as features for model training. An example would be to use the 10 features from the first or third hidden layer:


```{r}


#  Take the third hidden layer

train_features <- 
  h2o.deepfeatures(
    model_nn, 
    train_unsupervised, 
    layer = 3
    ) %>%
  as.data.frame() %>%
  mutate(card_fraud = as.factor(as.vector(train_unsupervised[, "CARD.x"]))) %>%
  as.h2o()

response="card_fraud"

features_dim <- setdiff(colnames(train_features), response)

model_nn_dim <- h2o.deeplearning(y = response,
                               x = features_dim,
                               training_frame =  train_features,
                               reproducible = TRUE, 
                               balance_classes = TRUE,
                               seed = 123,
                               hidden = c(10, 2, 10), 
                               epochs = 100,
                               activation = "Tanh")


h2o.saveModel(model_nn_dim, path="model_nn_dim", force = TRUE)

model_nn_dim

```

One way to explore the performance of this model is to plot the 'gains' chart.  Gain at a given percentile is the ratio of cumulative number of targets (e.g., frauds) up to that percentile to the total number of targets (e.g., frauds) in the entire data set.  Here is a gains chart of the model.

```{r fig.cap="Gains Chart of Autoencoder Layer-3 in Detecting Fraud with SOX Data"}



library(tidyverse)
library(knitr)
library(kableExtra)
library(ggplot2)

## Gains/Lift Tables

gains <- h2o.gainsLift(model_nn_dim,train_features, valid=F, xval=F)


gains %>% 
  ggplot(aes(y=gains$gain, x=gains$cumulative_data_fraction)) +
  geom_line() +
  xlab("percentile") + ylab("gain") +
  xlim(0,1)


```


```{r}

# For measuring model performance on test data, we need to 
# convert the test data to the same reduced dimensions as the # trainings data:


test_dim <- h2o.deepfeatures(model_nn, test, layer = 3)

h2o.predict(model_nn_dim, test_dim) %>%
  as.data.frame() %>%
  mutate(actual = as.vector(test[, "CARD.x"])) %>%
  group_by(actual, predict) %>%
  summarise(n = n()) %>%
  mutate(freq = n / sum(n))

``` 

This  looks quite good in terms of identifying fraud cases: 91% of fraud cases were identified! However, many non-fraud cases were also classified as fraud. In actual audit applications this would not be a good model, and it is necessary to look further to improve the sensitivity and specificity of predictions.. 


## Anomaly detection

We can also ask which instances were considered outliers or anomalies within our test data, using the `h2o.anomaly()` function. Based on the autoencoder model that was trained before, the input data will be reconstructed and for each instance, the mean squared error (MSE) between actual value and reconstruction is calculated for both class labels.


```{r fig.cap="Anomalies and Outliers in Detecting Fraud with SOX Data"}

anomaly <- h2o.anomaly(model_nn, test) %>%
  as.data.frame() %>%
  tibble::rownames_to_column() %>% 
  mutate(
    card_fraud = 
      as.factor(
        as.vector(
          test[, "CARD.x"]
          )))


mean_mse <- anomaly %>%
  group_by(card_fraud) %>%
  summarise(mean = mean(Reconstruction.MSE))


## This, we can now plot:

ggplot(anomaly, aes(x = as.numeric(rowname), y = Reconstruction.MSE, color = as.factor(card_fraud))) +
  geom_point(alpha = 0.3) +
  geom_hline(data = mean_mse, aes(yintercept = mean, color = card_fraud)) +
  scale_color_brewer(palette = "Set1") +
  labs(x = "instance number",
       color = "card_fraud")


```

The plot does not show a clear-cut classification into fraud and non-fraud cases but the mean MSE is definitely higher for fraudulent transactions than for regular ones.

We can now identify outlier instances by applying an MSE threshold for what we consider outliers. We could e.g. say that we consider every instance with an MSE > 0.02 (chosen according to the plot above) to be an anomaly/outlier.


```{r}


anomaly <- anomaly %>%
  mutate(outlier = ifelse(Reconstruction.MSE > 0.02, "outlier", "no_outlier"))

anomaly %>%
  group_by(card_fraud, outlier) %>%
  summarise(n = n()) %>%
  mutate(freq = n / sum(n)) 

```

For this dataset, outlier detection is insufficient to correctly classify fraudulent credit card transactions either.  This suggests that further analysis is necessary.  In this example, we will continue with supervised learning to attempt to use SOX data to predict credit card fraud.



## Pre-trained supervised model

In this section, the autoencoder model is used as a pre-training input for a supervised model. This model will use the weights from the autoencoder for model fitting.


```{r}

train_features <- h2o.deepfeatures(
  model_nn, 
  train_unsupervised, 
  layer = 2
  ) %>%
  as.data.frame() %>%
  mutate(card_fraud = as.vector(train_unsupervised[, "CARD.x"]))

```

```{r}



response <- "CARD.x"

features <- setdiff(colnames(train_supervised), response)

model_nn_2 <- h2o.deeplearning(y = response,
                               x = features,
                               training_frame = train_supervised,
                               pretrained_autoencoder  = "model_nn",
                               reproducible = TRUE, 
                               balance_classes = F,
                               ignore_const_cols = TRUE,
                               seed = 123,
                               hidden = c(10, 2, 10), 
                               epochs = 100,
                               activation = "Tanh")

model_nn_2

```



```{r}

pred <- as.data.frame(h2o.predict(object = model_nn_2, newdata = test)) %>%
  mutate(actual = as.vector(test[, "CARD.x"]))

pred %>%
  group_by(actual, predict) %>%
  summarise(n = n()) %>%
  mutate(freq = n / sum(n)) 

summary(pred)

```

This shows promise as the means are close (though the model tends to skew towards underprediction of fraud).  This suggests that it is worthwhile to further improve the model by e.g. performing grid search for hyperparameter tuning, going back to the original features, selecting different engineered features and perhaps exploring different algorithms. 



## Measuring model performance on highly unbalanced data

Because of the severe bias towards non-fraud cases, we can not use performance measures like accuracy or area under the curve (AUC), as they would give overly optimistic results based on the high percentage of correct classifications of the majority class.  An alternative to AUC is to use the precision-recall curve or the sensitivity (recall)-specificity curve. To calculate and plot these metrics, we can use the `ROCR` package. There are different ways to calculate the area under a curve (e.g., see the `PRROC` package for details).  In the next analysis I create a simple line-integral function that calculates the area between every consecutive points-pair of $x$ (i.e.$ x_1 - x_0, x_2 - x_1$, etc.) under the corresponding values of $y$.


```{r  fig.cap="Precision-Recall and Sensitifity-Specificity Curves in Detecting Fraud with SOX Data"}


library(ROCR)

line_integral <- function(x, y) {
  dx <- diff(x)
  end <- length(y)
  my <- (y[1:(end - 1)] + y[2:end]) / 2
  sum(dx * my)
} 

str(pred)
summary(pred)

prediction_obj <- prediction(pred$predict, pred$actual)


par(mfrow = c(1, 2))
par(mar = c(5.1,4.1,4.1,2.1))

# precision-recall curve
perf1 <- performance(prediction_obj, measure = "prec", x.measure = "rec") 

x <- perf1@x.values[[1]]
y <- perf1@y.values[[1]]
y[1] <- 0

plot(perf1, main = paste("Area Under the\nPrecision-Recall Curve:\n", round(abs(line_integral(x,y)), digits = 3)))

# sensitivity-specificity curve
perf2 <- performance(prediction_obj, measure = "sens", x.measure = "spec") 

x <- perf2@x.values[[1]]
y <- perf2@y.values[[1]]
y[1] <- 0

plot(perf2, main = paste("Area Under the\nSensitivity-Specificity Curve:\n", round(abs(line_integral(x,y)), digits = 3)))

```

Precision is the proportion of test cases predicted to be fraud that were indeed fraudulent (i.e. the true positive predictions), while recall or sensitivity is the proportion of fraud cases that were identified as fraud. And specificity is the proportion of non-fraud cases that are identified as non-fraud.

The precision-recall curve tells us the relationship between correct fraud predictions and the proportion of fraud cases that were detected (e.g. if all or most fraud cases were identified, we also have many non-fraud cases predicted as fraud and vice versa). The sensitivity-specificity curve thus tell us the relationship between correctly identified classes of both labels (e.g. if we have 100% correctly classified fraud cases, we will have no correctly classified non-fraud cases and vice versa).

We can also look at this a little bit differently, by manually going through different prediction thresholds and calculating how many cases were correctly classified in the two classes:

```{r  fig.cap="Search for Optimal Prediction Cutoff in Predicting Fraud with SOX Data"}



thresholds <- seq(from = 0, to = 1, by = 0.1)
pred_thresholds <- data.frame(actual = pred$actual)

for (threshold in thresholds) {
  
  prediction <- ifelse(pred$predict > threshold, 1, 0)
  prediction_true <- ifelse(pred_thresholds$actual == prediction, TRUE, FALSE)
  pred_thresholds <- cbind(pred_thresholds, prediction_true)

}

colnames(pred_thresholds)[-1] <- thresholds
pred_thresholds %>%
  gather(x, y, 2:ncol(pred_thresholds)) %>%
  group_by(actual, x, y) %>%
  summarise(n = n()) %>%
  ggplot(aes(x = as.numeric(x), y = n, color = actual)) +
    geom_vline(xintercept = 0.6, alpha = 0.5) +
    geom_line() +
    geom_point(alpha = 0.5) +
    theme_bw() +
    facet_wrap(actual ~ y, scales = "free", ncol = 2) +
    labs(x = "prediction threshold",
         y = "number of instances")

```

Figure 13's plots tell us that we can increase the number of correctly classified non-fraud cases without loosing correctly classified fraud cases when we increase the prediction threshold from the default 0.5 to 0.6:


```{r}

pred %>%
  mutate(predict = ifelse(pred$predict > 0.6, 1, 0)) %>%
  group_by(actual, predict) %>%
  summarise(n = n()) %>%
  mutate(freq = n / sum(n)) 
```

Our final model does not greatly improve on the previous models, suggesting at this point that more information is needed for accurate prediction.  Auditors in particular should have access to more extensive proprietary information than the publicly reported data in SOX.   This can conceivably be merged with the SOX data, while using the previous analysis to provide additional specificity and sensitivity in credit card fraud detection.


## Fama-French Risk Measures and Grid Search of Machine Learning Models to Augment Sarbanes-Oxley Information


Accounting and financial data are by their very nature non-linear and multicolinear, implying that cross-correlations and behavior of ensembles of variables can have significant impact on predictions.  Machine learning models can overcome such common problems with traditional statistics, because they can search for solutions in parameter spaces that are much larger.  Interpretation can sometimes be more difficult, but we provide a set of 'leverage' metrics for the influence of the input predictors that rank the importance of particular factors in the determination of credit card fraud.  Though these cannot be interpreted in the directly causal manner in which predictors and regression statistics can be used, they do indicate where changes in the environment, management policy and implementation will influence the occurrence of credit card fraud.  

This research trained and cross-validated the following algorithms (in the following order): three pre-specified XGBoost GBM (Gradient Boosting Machine) models, a fixed grid of GLMs, a default Random Forest (DRF), five pre-specified H2O GBMs, a near-default Deep Neural Net, an Extremely Randomized Forest (XRT), a random grid of XGBoost GBMs, a random grid of H2O GBMs, and a random grid of Deep Neural Nets. From this base, two Stacked Ensemble models were trained, then a comprehensive grid search explored hyperparameters for each algorithm, optimizing the F1 classification statistic.

Classification performance is typically ranked on specificity-selectivity, or alternatively precision and recall performance.  These are used in binary classification to study the output of some classifier ($C$). High precision means that an algorithm returned substantially more relevant results than irrelevant ones. High recall means that an algorithm returned most of the relevant results.  These are related to the sensitivity-specificity framework. Sensitivity is the same as recall, and measures the proportion of actual positives that are correctly identified.  Specificity  measures the proportion of actual negatives that are correctly identified.  All four are computed from the 'confusion matrix' -- a $2 \times 2$ matrix of accuracy in classification shown in table 4.  

```{r error=F, warning=F, message=F, echo=F,echo=F, fig.cap="The Confusion Matrix"}



tab_l <- matrix(c("",
           "condition positive",
           "condition negative",
           "predicted condition positive",
           "(TP) true positive",
           "(FP) false positive (type I error)",
           "predicted condition negative",
           "(FN) false negative (type II error)",
           "(TN) true negative"),
           nrow=3,
           ncol=3
           )

knitr::kable(tab_l,
             caption = "The Confusion Matrix"
             )

```





```{r error=F, warning=F, message=F, echo=F,fig.cap="Machine Learning Models Tested"}

library(tidyverse)
library(kableExtra)


  read.csv("/home/westland/Documents/SOX-AI/sox_only/mod_rank_so.csv") %>% 
  knitr::kable("latex", booktabs = T,
             caption = "Ranked Machine Learning Models Tested: SOX-PCH data"
             )


```



Conditional on the classifier $C$, these are computed:

- $precision = \frac{TP}{TP+FP} \rvert C$
- $recall = sensitivity =  \frac{TP}{TP+FN} \rvert C$
- $specificity =  \frac{TN}{TP+FN} \rvert C$

The receiver operating characteristic (ROC) curve for either precision-recall or sensitivity-specificity summarizes the performance of a classier over the range of $C$ and sometimes plotted in a $2-dimensional$ graph.  The integral of that curve between 0 and 1 is called the area under the cure (AUC) and summarizes performance of a model as a single number.

The $F1$ score (the harmonic mean of precision and sensitivity) is used to rank our classification models for whether or not credit card fraud has occurred, and is calculated as:

$F1 = \frac{2 \cdot TP}{2 \cdot TP + FP + FN}$

Table 5 provides the ranking of our models using the SOX-Privacy Clearinghouse (PCH) dataset.  The best performing model (table 6) based on area under the sensitivity-specificity curve (AUC) was a gradient boosting machine listed on the first line of table 5  This model had the following performance statistics.  


```{r error=F, warning=F, message=F, echo=F, fig.pos='H'}

library(tidyverse)
library(kableExtra)


  read.csv("/home/westland/Documents/SOX-AI/sox_only/best_mod_perf_so.csv") %>% 
  knitr::kable("latex", booktabs = T,
             caption = "Best Performing Model Statistics"
             )


```


For this model, the training confusion matrix is shown in table 7 and validation in table 8; the vertical identifiers represent actual outcomes (i.e., the response = 0/1) and horizontal identifiers represent predicted outcomes at the $F1$-optimal threshold (refer to table 4 which verbally describes each cell). 


```{r error=F, warning=F, message=F, echo=F,error=F, warning=F, message=F, echo=F, fig.cap="Best GBM model 'training' confusion matrix"}

library(tidyverse)
library(kableExtra)


  read.csv("/home/westland/Documents/SOX-AI/sox_only/conf_train_so.csv") %>% 
  knitr::kable(col.names = c("",
                           "0",
                           "1",
                           "Error",
                           "Rate"),
               caption = "Best Performing Model: Confusion matrix (train)"
               )


```

After validation, there is a slight trade-off of type I (.008 dropping to .005) for type II error (.13 rising to .20).  The closeness of the training and validation matrices confirm that the model is robust.  Furthermore, the overall statistics are very good, with sensitivity-specificity AUC of 99.3% and precision-recall AUC of 86% and Gini coefficient (i.e., AUC for the Lorenz curve) of 98.7%.  All other statistics were excellent as well.


```{r error=F, warning=F, message=F, echo=F,error=F, warning=F, message=F, echo=F, fig.cap="Best GBM model 5-fold 'validation' confusion matrix"}

library(tidyverse)
library(kableExtra)


  read.csv("/home/westland/Documents/SOX-AI/sox_only/conf_val_so.csv") %>% 
  knitr::kable(col.names = c("",
                           "0",
                           "1",
                           "Error",
                           "Rate"),
                 caption = "Best Performing Model: Confusion matrix (validate)"
               )


```

Table 9 presents the leverage of each predictor in the prediction of credit card fraud for the dataset.  This is a list of the most important variables, rescaled and ranked to reflect different ranges of variables, and to make them comparable.  Much of the predictive power is firm specific (as would be expected, since control weaknesses vary at the firm level).  What is surprising is that the fee-based predictors are much more important in predicting credit card fraud than are the results of SOX testing (which demonstrate very little predictive power in this model).   Unfortunately, the most important information seems to have been firm specific, with the next most important predictor having only half the influence of firm specific effects.  The most important non-firm specific predictors seem to be fees -- this is to be an expected correlate, since companies with internal control problems are likely to need both more auditing as well as be more susceptible to security breaches.  As generic predictors applicable to all firms in a set, audit fee information is more of a lagging indicator than a reliable predictor.   



```{r  echo=F,  fig.pos='H'}

library(tidyverse)
library(kableExtra)


  read.csv("/home/westland/Documents/SOX-AI/sox_only/last_vbl_importance.csv") %>% 
  knitr::kable("latex", booktabs=T,
               caption = "Variable leverage, best performing model on SOX-PCH data"
               )
               



```






# Fama-French Risk Factors

In asset pricing and portfolio management the Fama–French models estimate stock returns from various risk factors [@fama1993common]. 

$r = \alpha + R_f+\beta_{mkt} \cdot (R_m-R_f)+b_{smb} \cdot SMB + b_{hml} \cdot HML + \epsilon$

- $r$ is the portfolio's expected rate of return (**RET**)
- $\alpha$ is a market effect (**alpha**)
- $R_f$ is the risk-free return rate
- $R_m$ is the return of the market portfolio. (coeff: **b_mkt**)
- _SMB_ is _"Small [market capitalization] Minus Big"_  (coeff: **b_smb**)
- _HML_ is _"High [book-to-market ratio] Minus Low"_ (coeff: **b_hml**)
- _ivol_ is inferred volatility -- volatility that can be inferred from the market price using calculations similar to an options pricing model and solving for the volatility input
-  _tvol_ is theoretical volatility, inferred from calculations similar to options pricing using the Heston Model with a root-mean-square formula 
- $\epsilon$ is an error term with some presumed distribution  (**exret**)
- $R^2$ measures fit of the regression (**R2**)

The Fama–French model used three predictors (with an additional two being added in later work) -- (1) market risk, (2) the outperformance of small versus big companies, and (3) the out-performance of high book/market versus small book/market companies [@petkova2006fama]. Fama and French started with the observation that two classes of stocks have tended to do better than the market as a whole: (i) small caps and (ii) stocks with a high book-to-market ratio (B/P, customarily called value stocks, contrasted with growth stocks).  They then added two factors to CAPM to reflect a portfolio's exposure to these two classes [@fama1993common].  These factors are calculated with combinations of portfolios composed by ranked stocks and available historical market data. 

The Fama–French three-factor model explains over 90% of the diversified portfolios returns, compared with the average 70% given by the CAPM (within sample). They find positive returns from small size as well as value factors, high book-to-market ratio and related ratios. Examining $\beta$ and size, they find that higher returns, small size, and higher $\beta$ are all correlated. They then test returns for $\beta$, controlling for size, and find no relationship. Assuming stocks are first partitioned by size the predictive power of $\beta$ then disappears. They discuss whether  $\beta$ can be saved and the Sharpe-Lintner-Black model resuscitated by mistakes in their analysis, and find it unlikely.[@fama1992cross].  In 2015, Fama and French extended the model, adding profitability and investment, and further suggesting volatility measures. These models tend to do well in predicting US returns, but fall short in other markets.  For the purpose of this research, which is US-centric, the Fama-French metrics are relevant, and we adopted the three-factor model with two synthetic volatility metrics -- inferred and theoretical volatility. 

As we did previously, we trained and cross-validated algorithms in the following order: three pre-specified XGBoost GBM (Gradient Boosting Machine) models, a fixed grid of GLMs, a default Random Forest (DRF), five pre-specified H2O GBMs, a near-default Deep Neural Net, an Extremely Randomized Forest (XRT), a random grid of XGBoost GBMs, a random grid of H2O GBMs, and a random grid of Deep Neural Nets. From this base, two Stacked Ensemble models were trained.  A comprehensive grid search explored hyperparameters for each algorithm, optimizing the $F1$ classification statistic.

Classification performance was ranked on $F_1$ score (the harmonic mean of precision and sensitivity) for whether or not credit card fraud has occurred is calculated as:

$F_1 = \frac{2 \cdot TP}{2 \cdot TP + FP + FN}$

Table 10 provides the ranking of our models using the previous dataset with added Fama-French risk factors.  Because data was limited to publicly traded firms for the Fama-French risk factors, total firms in the database were reduced to around half the number available for the full SOX-Privacy Clearinghouse dataset.


```{r error=F, warning=F, message=F, echo=F,error=F, warning=F, message=F, echo=F, fig.cap="Machine Learning Models Tested"}

library(tidyverse)
library(kableExtra)


  read.csv("/home/westland/Documents/SOX-AI/sox+ff/model_rank.csv") %>% 
  knitr::kable("latex", booktabs = T,
               caption = "Machine Learning Models Tested: SOX-PCH + Fama-French risk factors"
               )


```



```{r error=F, warning=F, message=F, echo=F,error=F, warning=F, message=F, echo=F, fig.cap="GBM_grid__1_AutoML_20200301_090814_model_1 Performance Statistics"}

library(tidyverse)
library(kableExtra)


  read.csv("/home/westland/Documents/SOX-AI/sox+ff/best_mod_perf.csv") %>% 
  knitr::kable("latex", booktabs = T, 
               caption = "Performance statistics of best model"
               )


```



```{r error=F, warning=F, message=F, echo=F,error=F, warning=F, message=F, echo=F, fig.cap="GBM_grid__1_AutoML_20200301_090814_model_1 training confusion matrix"}

library(tidyverse)
library(kableExtra)


  read.csv("/home/westland/Documents/SOX-AI/sox+ff/conf_train.csv") %>% 
  knitr::kable(col.names = c("",
                           "0",
                           "1",
                           "Error",
                           "Rate"), 
               caption = "Best model: Confusion matrix (train)"
               )


```


```{r error=F, warning=F, message=F, echo=F,error=F, warning=F, message=F, echo=F, fig.cap="GBM_grid__1_AutoML_20200301_090814_model_1 5-fold validation confusion matrix"}

library(tidyverse)
library(kableExtra)


  read.csv("/home/westland/Documents/SOX-AI/sox+ff/conf_vali.csv") %>% 
  knitr::kable(col.names = c("",
                           "0",
                           "1",
                           "Error",
                           "Rate"), 
               caption = "Best model: Confusion matrix (validate)"
               )
               


```



```{r error=F, warning=F, message=F, echo=F,   fig.cap="Variable importance for best performing ML model "}


library(tidyverse)
library(kableExtra)


  read.csv("/home/westland/Documents/SOX-AI/sox+ff/vbl_impotnce.csv") %>% 
  knitr::kable("latex", booktabs=T,
               caption = "Variable importance for best performing ML model: SOX-PCH + Fama-French risk factors"
               )
               




```
