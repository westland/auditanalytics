---
title: "scan_curated_p159"
format: html
editor: visual
---

# Intelligence Scanning of Curated News Streams

Curated intelligence sources such as Google News, MacRumours, and other news feeds, offer prepackaged information that can be  My favorite is `Feedly` (stylized as `feedly`),  a news aggregator application that compiles news feeds from a variety of online sources for the user to customize and share with others. Feedly is emblematic of the sort of cloud based information services that are revolutionizing the audit process.

Start by going to https://developer.feedly.com/ and then page to https://feedly.com/v3/auth/dev to sign in, get a user ID and then follow steps to get your access token.  This requires either a “Pro” account or a regular account and you manually requesting OAuth codes.   Store it in your ~/.Renviron in FEEDLY_ACCESS_TOKEN.

Feedly will return your access token and refresh token, which looks like the tokens below, and which you can save on your computer, leaving you free to easily access Feedly content. 

___

    Congratulations, you have successfully generated a developer access token.

    Your user id is 448b1736-7c91-45d2-8a06-1cd797b12edc
    Your access token: 
    AwPMVtPDxTUC7FAKEIb6_9P:feedlydev

    (expires on 2019-03-15)

    Your refresh token (help): 
    AwPMVtPDxTUC44wAQ

    Do not share your tokens!
___



```{r eval=T ,message=F,error=F, warning=F}
knitr::opts_chunk$set(echo = TRUE)

  
#' Simplifying some example package setup for this non-pkg example
.pkgenv <- new.env(parent=emptyenv())
.pkgenv$token <- Sys.getenv("FEEDLY_ACCESS_TOKEN")

#' In reality, this is more complex since the non-toy example has to
#' refresh tokens when they expire.
.feedly_token <- function() {
  return(.pkgenv$token)
}


```


For the purposes of this example, consider a "stream" to be all the historical items in a feed.  Maximum "page size" (max number of items returned in a single call) is 1,000. For simplicity, there is a blanket assumption that if `continuation` is actually present, we can ask for a large number of items (e.g. 10,000)
    
    
```{r eval=F,message=F,error=F, warning=F}

devtools::install_github("hrbrmstr/seymour")
devtools::install_github("hrbrmstr/hrbrthemes")

library(seymour) 
library(hrbrthemes) 
library(tidyverse)


# Find writeups for seymour functions at https://github.com/hrbrmstr/seymour/tree/master/man  

## useful functions for extracting news are:
# feedly_search_contents: Search content of a stream
# feedly_search_title: Find feeds based on title, url or ‘#topic’
# feedly_stream: Retrieve contents of a Feedly “stream”
# feedly_subscribe: Subscribe to an RSS feed
# feedly_subscriptions: Retrieve Feedly Subscriptions


## use Feedly to check news about Apple Inc. and its products 
## (search "feedburner.com" and then the corporation names to see what is available)

apple_feed_id <- "feed/http://feeds.feedburner.com/MacRumors"

## Here is the stream function

feedly_stream(stream_id, 
              ranked = c("newest", "oldest"),  
              unread_only = FALSE, 
              count = 1000L, 
              continuation = NULL,   
              feedly_token = feedly_access_token())

apple_stream <- feedly_stream(apple_feed_id)
glimpse(apple_stream)


## Here is another function

feedly_search_contents(query, 
                       stream_id = NULL, 
                       fields = "all",   
                       embedded = NULL, 
                       engagement = NULL, 
                       count = 20L, 
                       locale = NULL,   
                       feedly_token = feedly_access_token())

f_search <-  
  feedly_search_contents(q = "ipod", 
                         stream_id = "apple_feed_id", 
                         fields = "keywords")
glimpse(f_search)

# preallocate space
streams <- vector("list", 10)
streams[1L] <- list(apple_stream)

# catch all of the content

idx <- 2L
while(length(apple_stream$continuation) > 0) {
  cat(".", sep="") #  progress bar, sort of
  feedly_stream(
    stream_id = apple_feed_id,
    ct = 10000L,
    continuation = apple_stream$continuation
  ) -> rb_stream
  streams[idx] <- list(apple_stream)
  idx <- idx + 1L
}
cat("\n")


str(streams, 1)
str(streams[[1]], 1)
glimpse(streams[[1]]$items)

```

Feedly curates numerous news and blog outlets, which distribute a broad array of news items from formal press publications that would have a bearing on the conduct of an audit. I suggest you use the various `lubridate`, `ggplot` and `tidyverse` tools to analyze and present any insights from this larger dataset.

There are many sources of curated news for conducting analytical reviews in an audit, each with its own merits.  The largest consolidator of curated news on the web is arguably Google.  The `newsAPI` package is used to access Google's News API using R.


```{r eval=F,message=F,error=F, warning=F}


## install script
if (!"devtools" %in% installed.packages()) {
  install.packages("devtools")
}
devtools::install_github("mkearney/newsAPI")

## load package
library(newsAPI)

# go to newsapi.org and register to get an API key.
# save the key as an environment variable

## my obscured key
NEWSAPI_KEY <- "079ee9e373894dcfb9a062a85c4e1e7e"

## save to .Renviron file
cat(
  paste0("NEWSAPI_KEY=", NEWSAPI_KEY),
  append = TRUE,
  fill = TRUE,
  file = file.path("~", ".Renviron")
)

## install script
if (!"devtools" %in% installed.packages()) {
  install.packages("devtools")
}
devtools::install_github("mkearney/newsAPI")

src <-  get_sources(category = "", language = "en", country = "", apiKey = NEWSAPI_KEY,
  parse = TRUE)

## load package
library(newsAPI)

df <- lapply(src$id, get_articles,apiKey=NEWSAPI_KEY)

## collapse into single data frame
df <- do.call("rbind", df)

## additional functions allow the parsing of streamed news articles

# get_sources(
  #category = "", language = "", country = "", apiKey = NEWSAPI_KEY,   parse = TRUE)


# x <- get_sources(
  #category = "", language = "", country = "", apiKey = NEWSAPI_KEY,   parse = TRUE)
#parse_sources(x)

# y <- get_articles(
  # source, sortBy = "top", apiKey = NEWSAPI_KEY, parse = TRUE)
#parse_articles(y)

# source:Name of news source.
# sortBy: Name of sorting mechanism must be one of 
   ## latest, top, or popular. Certain methods only work for certain news sources.

# apiKey: Character string API token. Default is to grab it from user R environ.
# parse: Logical indicating whether to parse response object to data frame.

 # where x is a response object from get_sources


```

An alternative to Google News is Feedly's (stylized as `feedly`) news aggregator application that compiles news feeds from a variety of online sources -- especially firm specific, technical and financial news feeds. 

Start by going to https://developer.feedly.com/ and then page to https://feedly.com/v3/auth/dev to sign in, get a user ID and then follow steps to get your access token.  This requires either a “Pro” account or a regular account and you manually requesting OAuth codes.   Store it in your ~/.Renviron in FEEDLY_ACCESS_TOKEN.  Feedly will return your access token and refresh token, which looks like the tokens below, and which you can save on your computer, leaving you free to easily access Feedly content. 

Feedly is a news aggregator application for various web browsers and mobile devices running 'iOS' and 'Android', also available as a cloud-based service. It compiles news feeds from a variety of online sources for the user to customize and share with others. Methods are provided to retrieve information about and contents of 'Feedly' collections and streams.

Neither `feedly_search()` nor `feedly_stream()` require authentication (i.e. you do not need a developer token) to retrieve the contents of the API call. For `feedly_stream()` You do need to know the Feedly-structured feed id which is (generally) feed/FEED_URL (e.g. feed/http://feeds.feedburner.com/RBloggers).

I've generally found Feedly to be more useful than Google News for business intelligence scanning, because it is less hampered by throttling, and the curation extends to industry specific feeds (rather than Google's algorithmic guess about the topic),

In the following example, consider a "stream" to be all the historical items in a feed.  Maximum "page size" (max number of items returned in a single call) is 1,000. For simplicity, there is a blanket assumption that if `continuation` is actually present, we can ask for a large number of items (e.g. 10,000)

```{r}

library(kableExtra)

read.csv("~/audit_analytics_book/aaa_chapters/tables/feedly_functions.csv") %>% 
  kable("latex", booktabs = T) %>% 
  kable_styling()


```

    
    
```{r eval=F,message=F,error=F, warning=F}

# devtools::install_github("hrbrmstr/seymour")
# devtools::install_github("hrbrmstr/hrbrthemes")

library(seymour) 
library(hrbrthemes) 
library(tidyverse)


# Find writeups for seymour functions at https://github.com/hrbrmstr/seymour/tree/master/man  

## useful functions for extracting news are:
# feedly_search_contents: Search content of a stream
# feedly_search_title: Find feeds based on title, url or ‘#topic’
# feedly_stream: Retrieve contents of a Feedly “stream”
# feedly_subscribe: Subscribe to an RSS feed
# feedly_subscriptions: Retrieve Feedly Subscriptions


## Let's check what is happening at Apple 
## (search Google's "feedburner.com" and then the corporation names to see what is available)


## the following should retreive your Feedly access token, but if not, 
## you can log into the site and request the token.

token <- feedly_access_token()

feedly_search_title("roland")

# prefix the URL with 'feed/'
music_feed_id <- "feed/http://feeds.feedburner.com/MusicRadar"
music_feed_id_2 <- "feed/http://feeds.feedburner.com/MusicTech"


## Here is the stream function
feedly_stream(stream_id,
              unt = 1000L, 
              continuation = NULL,   
              feedly_token = feedly_access_token())

music_stream <- feedly_stream(music_feed_id_2)
glimpse(music_stream)


## Here is another function
feedly_search_contents(query, 
                       stream_id = NULL, 
                       fields = "all",   
                       embedded = NULL, 
                       engagement = NULL, 
                       count = 20L, 
                       locale = NULL,   
                       feedly_token = feedly_access_token())

f_search <-  feedly_search_contents(q = "ipod", stream_id = "music_feed_id", fields = "keywords")
glimpse(f_search)

# preallocate space
streams <- vector("list", 10)
streams[1L] <- list(music_stream)

# catch all of the content

idx <- 2L
while(length(music_stream$continuation) > 0) {
  cat(".", sep="") #  progress bar, sort of
  feedly_stream(
    stream_id = music_feed_id,
    ct = 10000L,
    continuation = music_stream$continuation
  ) -> rb_stream
  streams[idx] <- list(music_stream)
  idx <- idx + 1L
}
cat("\n")


str(streams, 1)
str(streams[[1]], 1)
glimpse(streams[[1]]$items)

```


API datastreams from social networks, blogs and other Internet resources tend to be best for qualitative intelligence scanning.  They can alert the auditor to information that would not appear in financial news, or in the accounting statements and transactions.  Such information is an essential part of the analytical review process, but until the advent of Internet accessible resources and automated tools provided by R, has not been accessible to auditors in a cost-effective way.
