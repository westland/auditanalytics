---
title: "scan_uncurated_p155"
format: html
editor: visual
---

## Scanning of Uncurated News Sources from Social Networks


Twitter is arguably the most important of all the social networks for market analysis.   Facebook, MySpace, Instagram, Snapchat LinkedIn, WeChat, and so forth potentially could offer more useful marketing information, but regulations either prohibit the distribution of data completely (e.g., LinkedIn) or the significantly limit it to just the people you may know directly (Facebook).  This section shows you how to gain access to Twitter's API (application programming interface).  

Twitter has around 130 million daily active users, a number that vastly dwarfs any of the finance specific social platforms (e.g., StockTwits) which have usage measured in hundreds of thousand.  Even though it is not finance specific, the volume of financial information exchanged on Twitter is substantially larger than on bespoke finance platforms. In addition, Twitter is more likely to report the market, competitor and product specific problems that tend to drive valuations today.  I use the `rtweet` package here to extract information from Twitter. 

The "#" symbol is used to refer to individuals on Twitter. A # symbol before a word tells Twitter to "index" all transactions that have this hashtag. Twitter accumulates all posts that have it and retrieves them more quickly than would search with a query engine.

The "@" symbol is used to refer to individuals on Twitter. It is combined with a username and inserted into tweets to refer to that person or send them a public message. When @ precedes a username, it automatically gets linked to that user's profile page.  Users may be individuals or firms.


It is not necessary to receive a token to use `rtweet`; only a user account is required.  But the `OAuth` authentication gives access to more functions on Twitter, such as posting. If you need full access, go to *https://developer.twitter.com* (Twitter Developer Site) create an app and apply for a consumer key and consumer secret; as the 'Callback URL' enter:* http://127.0.0.1:1410*.   

The authentication process of Twitter involves creating a *Twitter app* and doing a *handshake*.  You have to *handshake* every time you want to get data from Twitter with R.  Since Twitter released the Version 1.1 of their API an OAuth handshake is necessary for every request you do.  The following steps will get you onto the Twitter API:

1. Create an *app* at Twitter:  Go to *apps.twitter.com/* and log in with your Twitter Account. from your Profile picture in the upper right corner select the drop-down menu and look for *“My Applications”*. Click on it and then on *“Create new application”*.  As the Callback URL enter:* http://127.0.0.1:1410*.  Click on *Create* you´ll get redirected to a screen with all the OAuth setting of your new App.

1. Use the *setup_twitter_OAuth()* function which uses the *httr* package. Get your *api_key* and your *api_secret* as well as your *access_token* and *access_token_secret* from your app settings on Twitter (click on the *“API key”* tab to see them).  Here is an example.

```{r eval=T,message=F,error=F, warning=F}

## install devtools package if it's not already
if (!requireNamespace("devtools", quietly = TRUE)) {
  install.packages("devtools")
}

## install dev version of rtweet from github
devtools::install_github("mkearney/rtweet")
install.packages("maps")
## load rtweet package
library(rtweet)
library(maps)


## access token method: create token and save it as an environment variable
create_token(
  app = "Test_of_the_API_platform",
  consumer_key = 'AWsZc3pjFsgAFlBK4OHRlyGtK',
  consumer_secret =  'DTRvorcjSaQQ1goWzynZ2tc226mgRvQ1JPxGur7nQMTesuXw3z',
  access_token = '14122740-FWlOwlo4qvhiy6oTcRypgVaIyvmlg1OZLudAToO6c',
  access_secret = 'sYjzQMjFKQFvMVRCU9gYx7bOteiS4XCoLvCgodTJZVm7y')

## Google API key for accessing geo location data through Google Maps
westland_api <- 'AIzaSyCErk3aBmPoG1FAKEqNUz6elhD6ZrR2MQtN7W0'


# To test your authentication, search for 18000 tweets using the rstats hashtag

rt <- search_tweets(
  "#rstats", n = 18000, include_rts = FALSE
)


```


#### Example: Extracting tweets about General Motors and the Auto Industry


```{r eval=T,message=F,error=F, warning=F}

library(tidyverse)
library(rtweet)




##Query used to select and customize streaming collection method. 
## There are four possible methods. 

## (1) The default, q = "", 
## returns a small random sample of all publicly available Twitter statuses. 

## (2) To filter by keyword, provide 
## a comma separated character string with the desired phrase(s) and keyword(s). 

## (3) Track users by providing a comma separated list of user IDs or screen names. 

## (4) Use four latitude/longitude bounding box points to stream by geo location.
##This must be provided via a vector of length 4, e.g., c(-125, 26, -65, 49).


stream_tweets(
  q = "auto, car, general motors, GM",
  timeout = 100,    ## the number of seconds that you will access. 
                    ## Max 18000 tweets / 15 min
  parse = FALSE,    ## we'll do this later
  file_name = "tweetsaboutGM.json"
)

## read in the data as a tidy tbl data frame
djt <- parse_stream("tweetsaboutGM.json")

djt <- djt[,3:6]  ## just a few things we'd like to see
glimpse(djt)

# To get an idea of what you should be seeing before you 
# actually signup for a Twitter OAuth code, 
# you can bring in the dataset *trump_tweet.RData 
# provided with this workout

```



#### Example: zxtracting Tweets about Roland Musical Instruments and their Industry

For the following examples of R code, we will be interested in Internet intelligence scanning to support a presumed analytical review of the Roland Corporation.  Headquartered in Hamamatsu, Japan, Roland's 3000 employees design, manufacture and market electronic musical instruments. The firm was publicly traded until 2014 as TYO: 7944.  Since it is not a US publicly listed firm, the auditor would not have any access to SEC-EDGAR filings.


```{r eval=F, ,message=F,error=F, warning=F}

library(rtweet)
library(httpuv)

#Twitter rate limits cap the number of search results returned to 18,000 every 15 minutes. 
## To request more than that,  set retryonratelimit = TRUE 
## and rtweet will wait for rate limit resets for you.  
## Here we search for 18000 tweets using the Roland hashtag


roland_tweets <- search_tweets(
  "#Roland", n = 18000, include_rts = FALSE, retryonratelimit = TRUE
)


## plot time series of tweets
roland_tweets %>%
  ts_plot("3 hours") +
  ggplot2::theme_minimal() +
  ggplot2::theme(plot.title = ggplot2::element_text(face = "bold")) +
  ggplot2::labs(
    x = NULL, y = NULL,
    title = "Frequency of #Roland Twitter statuses from past 9 days",
    subtitle = "Twitter status (tweet) counts aggregated using three-hour intervals",
    caption = "\nSource: Data collected from Twitter's REST API via rtweet"
  )

```


I can perform a quick sentiment analysis of the Roland tweets using the *NRC Word-Emotion Association* lexicon (available in R in the `textdata` library) which associates words with ten sentiments:

    - positive
    - negative
    - anger
    - anticipation
    - disgust
    - fear
    - joy
    - sadness
    - surprise
    - trust


```{r eval=F, ,message=F,error=F, warning=F}

library(tidytext)
library(tidyverse)
library(textdata)
library(ggplot2)

reg <- "([^A-Za-z\\d#@']|'(?![A-Za-z\\d#@]))"
tweet_words <- 
  roland_tweets %>%
  select(user_id,source,created_at,text) %>% 
  filter(!str_detect(text, '^"')) %>%
  mutate(text = str_replace_all(text, "https://t.co/[A-Za-z\\d]+|&amp;", "")) %>%
  unnest_tokens(word, text, token = "regex", pattern = reg) %>%
  filter(!word %in% stop_words$word,
         str_detect(word, "[a-z]"))


sentmnt <- inner_join(
  get_sentiments("nrc"),
  tweet_words, 
  by="word") %>% 
  count(sentiment)  


ggplot(sentmnt, aes(sentiment, n)) + 
  geom_col() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  xlab("Sentiment") +
  ylab("Frequency expressed in tweets")
 

```

We see from this brief analysis that sentiment is overall very positive for Roland.  This clearly provides some insight into the sentiments of consumers and investors, but there is an important caveat.  Like most social platforms, Twitter curates very little, and the quality of information conveyed about businesses is highly variable.  It is best used to identify potential problems or exceptions that may not appear in other sources.  The auditor will want to augment Twitter analysis with curated news sources, and this is the subject of the next section.
 
