---
title: "sent_analysis_p145"
format: html
editor: visual
---

## Sentiment analysis with tidy data 

When human readers approach a text, we use our understanding of the emotional intent of words to infer whether a section of text is positive or negative, or perhaps characterized by some other more nuanced emotion like surprise or disgust. We can use the tools of text mining to approach the emotional content of text programmatically.  We start by representing text in R's "tidy" structure:

- Each variable is a column 
- Each observation is a row
- Each type of observational unit is a table

We thus define the tidy text format as being a table with one-token-per-row. A token is a meaningful unit of text, such as a word, that we are interested in using for analysis, and tokenization is the process of splitting text into tokens. This one-token-per-row structure is in contrast to the ways text is often stored in current analyses, perhaps as strings or in a document-term matrix. For tidy text mining, the token that is stored in each row is most often a single word, but can also be an n-gram, sentence, or paragraph. R's `tidytext` package provides functionality to tokenize by commonly used units of text like these and convert to a one-term-per-row format.

One way to analyze the sentiment of a text is to consider the text as a combination of its individual words and the sentiment content of the whole text as the sum of the sentiment content of the individual words. This is not the only way to approach sentiment analysis, but it is an often-used approach, and an approach that naturally takes advantage of the tidy tool ecosystem.  The tidytext package contains several sentiment lexicons in the sentiments dataset.  For example, consider the following code chunk.


```{r eval=T,message=F,error=F, warning=F}
library(tidytext)
sentiments
```


The three general-purpose lexicons are

- `bing` from Bing Liu and collaborators at University of Illinois - Chicago, 
- `AFINN` from Finn Årup Nielsen, and
- `nrc` from Saif Mohammad and Peter Turney.

All three of these lexicons are based on unigrams, i.e., single words. These lexicons contain many English words and the words are assigned scores for positive/negative sentiment, and also possibly emotions like joy, anger, sadness, and so forth.  All three were constructed via either crowdsourcing (using, for example, Amazon Mechanical Turk) or by the labor of one of the authors, and were validated using some combination of crowdsourcing again, restaurant or movie reviews, or Twitter data. 

The nrc lexicon categorizes words in a binary fashion (“yes”/“no”) into categories of positive, negative, anger, anticipation, disgust, fear, joy, sadness, surprise, and trust. 

The `bing` lexicon categorizes words in a binary fashion into positive and negative categories. 

The `AFINN` lexicon assigns words with a score that runs between -5 and 5, with negative scores indicating negative sentiment and positive scores indicating positive sentiment. 

All of this information is tabulated in the sentiments dataset, and tidytext provides a function get_sentiments() to get specific sentiment lexicons without the columns that are not used in that lexicon.

```{r eval=T,message=F,error=F, warning=F}

get_sentiments("afinn")

get_sentiments("bing")

get_sentiments("nrc")

```



There are also some domain-specific sentiment lexicons available, constructed to be used with text from a specific content area -- e.g., for accounting and finance.  Dictionary-based methods like the ones we are discussing find the total sentiment of a piece of text by adding up the individual sentiment scores for each word in the text.  Not every English word is in the lexicons because many English words are pretty neutral. It is important to keep in mind that these methods do not take into account qualifiers before a word, such as in “no good” or “not true”; a lexicon-based method like this is based on unigrams only. 

One last caveat is that the size of the chunk of text that we use to add up unigram sentiment scores can have an effect on results. A text the size of many paragraphs can often have positive and negative sentiment averaged out to about zero, while sentence-sized or paragraph-sized text often works better.

With data in a tidy format, sentiment analysis can be done as an inner join. This is another of the great successes of viewing text mining as a tidy data analysis task; much as removing stop words is an antijoin operation, performing sentiment analysis is an inner join operation.  Let’s look at the words with a joy score from the NRC lexicon.  For this example, we capture an HTML formatted General Motors' 10-K report for 2017 from  SEC's ECGAR database, demonstrating that HTML documents may be used in the workpapers, as well as those in XBRL format.



```{r eval=T, error=F, message=F, warning=F, tidy=T}

library(htm2txt)
library(kableExtra)
library(tokenizers)
library(wordcloud)
library(tidyverse)
library(tidytext)

txt <- gettxt("https://www.sec.gov/Archives/edgar/data/1467858/000146785818000022/gm201710k.htm", encoding = "UTF-8") 

text_stuff <- htm2txt(txt) %>% 
 tokenize_words() %>% 
  unlist() %>% 
  as.data.frame()

colnames(text_stuff) <- "word"

stuff_sentiment <- text_stuff %>%
  inner_join(get_sentiments("bing"), by="word") 

text_stuff %>%
  anti_join(get_stopwords()) %>%
  inner_join(stuff_sentiment) %>% 
  count(word)%>%
  with(wordcloud(word, colors=rainbow(3), rot.per=.15, n, max.words = 1000))


net_sentiment <- text_stuff %>%
  inner_join(get_sentiments("bing"), by="word") %>%
  count(sentiment) %>%
  spread(sentiment, n, fill = 0) %>%
  mutate(net_positive = positive - negative,
         proportion__positive = positive / negative - 1) 


net_sentiment %>% 
  kable(longtable=T,"latex", booktabs = T) %>% 
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), full_width = F, font_size=10) 

```



The size of a word’s text here is in proportion to its frequency within its sentiment. We can use this visualization to see the most important positive and negative words, but the sizes of the words are not comparable across sentiments.

In other functions, such as `comparison.cloud()`, you may need to turn the data frame into a matrix with `reshape2`’s `acast()`. Let’s do the sentiment analysis to tag positive and negative words using an inner join, then find the most common positive and negative words. Until the step where we need to send the data to `comparison.cloud()`, this can all be done with joins, piping, and `dplyr` because our data is in tidy format.


```{r eval=T,message=F,error=F, warning=F}

library(reshape2)
library(wordcloud)

stuff_sentiment %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = TRUE) %>%
  acast(word ~ sentiment, value.var = "n", fill = 0) %>%
  comparison.cloud(colors = c("gray20", "gray80"),
                   max.words = 100)

```



With several options for sentiment lexicons, you might want some more information on which one is appropriate for your purposes. Let’s use all three sentiment lexicons and examine how the sentiment changes.  I use `inner_join()` to calculate the sentiment in different ways.  The `AFINN` lexicon measures sentiment with a numeric score between -5 and 5, while the other two lexicons categorize words in a binary fashion, either positive or negative.   Use integer division (`%/%`) to define larger sections of text that span multiple lines, and we can use the same pattern with the `tidyverse` library's pipe (`%>%`), `count()`, `spread()`, and `mutate()` to find sentiment.


```{r eval=T, message=F, warning=F, error=F}


library(tidyverse)
library(Hmisc)


cat("\n\n afinn")

get_sentiments("afinn") %>% 
  describe()
  
cat("\n\n nrc")
get_sentiments("nrc") %>% 
     filter(sentiment %in% c("positive", 
                             "negative")) %>% 
     describe()

cat("\n\n bing")
get_sentiments("bing") %>% 
       filter(sentiment %in% c("positive", 
                             "negative"))  %>% 
       describe()


```


Both `bing` and `NEC` lexicons have more negative than positive words, but the ratio of negative to positive words is higher in the `bing` lexicon than the `NRC` lexicon. This will contribute to the effect we see in the plot above, as will any systematic difference in word matches. 

One advantage of having the data frame with both sentiment and word is that we can analyze word counts that contribute to each sentiment. By implementing `count()` here with arguments of both `word` and `sentiment`, we find out how much each word contributed to each sentiment.

```{r eval=T,message=F,error=F, warning=F}

bing_word_counts <- stuff_sentiment %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = TRUE) %>%
  ungroup()

head(bing_word_counts)

```


This can be shown visually, and we can pipe (`%>%`) straight into `ggplot2`, if we like, because of the way we are consistently using tools built for handling tidy data frames.

```{r eval=T,message=F,error=F, warning=F}

bing_word_counts %>%
  group_by(sentiment) %>%
  top_n(10) %>%
  ungroup() %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, scales = "free_y") +
  labs(y = "Contribution to sentiment",
       x = NULL) +
  coord_flip()

```


This lets us spot an anomaly in the sentiment analysis; the words  “loss”, "debt" and "fair" are generic words used to describe many accounting situations and accounts.  If it were appropriate for our purposes, we could easily add “loss” to a custom stop-words list using `bind_rows()`. We could implement that with a strategy such as this.


```{r eval=T,message=F,error=F, warning=F}

library(tidytext)
library(tidyverse)
library(wordcloud)

custom_stop_words <- 
  bind_rows(tibble(word = c("loss", "debt", "fair"), 
  lexicon = c("custom")), 
  get_stopwords())

head(custom_stop_words)

## here is our prior wordcloud with custom_stop_words

text_stuff %>% 
  anti_join(custom_stop_words) %>% 
  inner_join(stuff_sentiment) %>% 
  count(word) %>%
  with(wordcloud(word, 
                 colors = rainbow(3), 
                 rot.per = 0.15, n, 
                 max.words = 1000)
       )

```


Much useful work can be done by tokenizing at the word level, but sometimes it is useful or necessary to look at different units of text. For example, some sentiment analysis algorithms look beyond only unigrams (i.e. single words) to try to understand the sentiment of a sentence as a whole. These algorithms try to understand that "I am not having a good day" is a sad sentence, not a happy one, because of negation. R packages included `coreNLP`, `cleanNLP`, and `sentimentr` are examples of such sentiment analysis algorithms. For these, we may want to tokenize text into sentences.

